{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Github**","metadata":{}},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\npersonal_token = user_secrets.get_secret(\"GITHUB_PAT\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-18T10:30:08.643353Z","iopub.execute_input":"2025-11-18T10:30:08.643721Z","iopub.status.idle":"2025-11-18T10:30:08.783472Z","shell.execute_reply.started":"2025-11-18T10:30:08.643698Z","shell.execute_reply":"2025-11-18T10:30:08.78261Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"!git clone https://{personal_token}@github.com/Mohamed-Mohamed-Ibrahim/NLP-lab-2.git /kaggle/working/NLP-lab-2","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-18T10:30:08.785212Z","iopub.execute_input":"2025-11-18T10:30:08.785573Z","iopub.status.idle":"2025-11-18T10:30:10.143451Z","shell.execute_reply.started":"2025-11-18T10:30:08.785543Z","shell.execute_reply":"2025-11-18T10:30:10.142093Z"}},"outputs":[{"name":"stdout","text":"Cloning into '/kaggle/working/NLP-lab-2'...\nremote: Enumerating objects: 93, done.\u001b[K\nremote: Counting objects: 100% (93/93), done.\u001b[K\u001b[K\nremote: Compressing objects: 100% (67/67), done.\u001b[K\nremote: Total 93 (delta 50), reused 60 (delta 24), pack-reused 0 (from 0)\u001b[K\nReceiving objects: 100% (93/93), 2.38 MiB | 18.47 MiB/s, done.\nResolving deltas: 100% (50/50), done.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"%cd /kaggle/working/NLP-lab-2","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-18T10:30:10.144981Z","iopub.execute_input":"2025-11-18T10:30:10.145321Z","iopub.status.idle":"2025-11-18T10:30:10.157899Z","shell.execute_reply.started":"2025-11-18T10:30:10.14528Z","shell.execute_reply":"2025-11-18T10:30:10.156057Z"}},"outputs":[{"name":"stdout","text":"/kaggle/working/NLP-lab-2\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"!git checkout word2vec","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-18T10:30:10.162978Z","iopub.execute_input":"2025-11-18T10:30:10.163829Z","iopub.status.idle":"2025-11-18T10:30:10.361341Z","shell.execute_reply.started":"2025-11-18T10:30:10.163775Z","shell.execute_reply":"2025-11-18T10:30:10.360101Z"}},"outputs":[{"name":"stdout","text":"Branch 'word2vec' set up to track remote branch 'word2vec' from 'origin'.\nSwitched to a new branch 'word2vec'\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"# **Import Libraries**","metadata":{}},{"cell_type":"code","source":"# === Standard Library ===\nimport os\nimport random\nimport string\nimport warnings\nfrom collections import Counter\n\n# === Third-Party Libraries ===\nimport pickle\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom datasets import load_dataset\nfrom sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom tqdm import tqdm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-18T10:30:10.362639Z","iopub.execute_input":"2025-11-18T10:30:10.362909Z","iopub.status.idle":"2025-11-18T10:30:19.75Z","shell.execute_reply.started":"2025-11-18T10:30:10.362885Z","shell.execute_reply":"2025-11-18T10:30:19.748995Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"# **Ignore Warnings**","metadata":{}},{"cell_type":"code","source":"warnings.filterwarnings('ignore')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-18T10:30:19.750987Z","iopub.execute_input":"2025-11-18T10:30:19.751589Z","iopub.status.idle":"2025-11-18T10:30:19.756871Z","shell.execute_reply.started":"2025-11-18T10:30:19.751556Z","shell.execute_reply":"2025-11-18T10:30:19.755487Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"# **Define Working Directory**","metadata":{}},{"cell_type":"code","source":"os.makedirs(\"word2vec\", exist_ok = True); ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-18T10:30:19.758025Z","iopub.execute_input":"2025-11-18T10:30:19.758387Z","iopub.status.idle":"2025-11-18T10:30:19.782878Z","shell.execute_reply.started":"2025-11-18T10:30:19.758354Z","shell.execute_reply":"2025-11-18T10:30:19.781597Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"%cd word2vec","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-18T10:30:19.784534Z","iopub.execute_input":"2025-11-18T10:30:19.785016Z","iopub.status.idle":"2025-11-18T10:30:19.808829Z","shell.execute_reply.started":"2025-11-18T10:30:19.784971Z","shell.execute_reply":"2025-11-18T10:30:19.807566Z"}},"outputs":[{"name":"stdout","text":"/kaggle/working/NLP-lab-2/word2vec\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"# **Load Dataset**","metadata":{}},{"cell_type":"code","source":"def load_corpus(corpus_name = 'lhoestq/conll2003'): \n    all_sentences = []\n    dataset = load_dataset(corpus_name)\n\n    for split in ['train', 'validation', 'test']:\n        for example in dataset[split]:\n            cleaned_tokens = []\n            for token in example['tokens']:\n                cleaned_tokens.append(token.lower())\n            all_sentences.append(cleaned_tokens)\n\n    return all_sentences","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-18T10:30:19.809843Z","iopub.execute_input":"2025-11-18T10:30:19.810147Z","iopub.status.idle":"2025-11-18T10:30:19.830406Z","shell.execute_reply.started":"2025-11-18T10:30:19.810119Z","shell.execute_reply":"2025-11-18T10:30:19.829478Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"corpus = load_corpus('lhoestq/conll2003')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-18T10:30:19.834158Z","iopub.execute_input":"2025-11-18T10:30:19.834764Z","iopub.status.idle":"2025-11-18T10:30:25.377346Z","shell.execute_reply.started":"2025-11-18T10:30:19.834545Z","shell.execute_reply":"2025-11-18T10:30:25.376401Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"dataset_infos.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bcd60cbee6ac4f609ab202e71bca41b7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"data/train-00000-of-00001.parquet:   0%|          | 0.00/1.07M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ab01a784caa042c3b153c56266767a85"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"data/validation-00000-of-00001.parquet:   0%|          | 0.00/281k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b01595c7251b4c5bbf46243fc3e6da20"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"data/test-00000-of-00001.parquet:   0%|          | 0.00/259k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"422318a2c78046d783be0ba3f48ee5d8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/14041 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cb1f3238ef8f4355acf859f1cc4e4176"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/3250 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"15238f223d8f465f8cff0c2e3323c366"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/3453 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b731f6e63e65497c8d8f279c8d40f660"}},"metadata":{}}],"execution_count":10},{"cell_type":"markdown","source":"# **Define Parameters**","metadata":{}},{"cell_type":"code","source":"min_count = 2\nembed_dim = 100\ncontext_size = 5\nnum_negative_samples = 5\nepochs = 100\nstart_alpha = 0.025\nmin_alpha = 0.0001","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-18T10:30:25.378443Z","iopub.execute_input":"2025-11-18T10:30:25.378749Z","iopub.status.idle":"2025-11-18T10:30:25.383768Z","shell.execute_reply.started":"2025-11-18T10:30:25.378728Z","shell.execute_reply":"2025-11-18T10:30:25.382721Z"}},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":"# **Construct Word–Index Dictionaries**","metadata":{}},{"cell_type":"code","source":"def build_vocab_mapping(corpus, min_count = 2):\n    \n    word_counts = Counter()\n    for sentence in corpus:\n        word_counts.update(sentence)\n    \n    vocab = [word for word, count in word_counts.items() if count >= min_count]\n    \n    word_to_idx = {word: idx for idx, word in enumerate(vocab)}\n    idx_to_word = {idx: word for idx, word in enumerate(vocab)}\n\n    word_freqs = [word_counts[word] for word in vocab]\n    \n    return word_to_idx, idx_to_word, word_counts, len(vocab)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-18T10:30:25.384891Z","iopub.execute_input":"2025-11-18T10:30:25.385274Z","iopub.status.idle":"2025-11-18T10:30:25.419273Z","shell.execute_reply.started":"2025-11-18T10:30:25.385244Z","shell.execute_reply":"2025-11-18T10:30:25.418046Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"word_to_idx, idx_to_word, word_counts, vocab_size = build_vocab_mapping(corpus, min_count)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-18T10:30:25.42075Z","iopub.execute_input":"2025-11-18T10:30:25.42158Z","iopub.status.idle":"2025-11-18T10:30:25.565268Z","shell.execute_reply.started":"2025-11-18T10:30:25.421503Z","shell.execute_reply":"2025-11-18T10:30:25.564351Z"}},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":"# **Sigmoid Function**","metadata":{}},{"cell_type":"code","source":"def sigmoid(x):\n    return 1.0 / (1.0 + np.exp(-np.clip(x, -6, 6)))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-18T10:30:25.566169Z","iopub.execute_input":"2025-11-18T10:30:25.566437Z","iopub.status.idle":"2025-11-18T10:30:25.571392Z","shell.execute_reply.started":"2025-11-18T10:30:25.566418Z","shell.execute_reply":"2025-11-18T10:30:25.570406Z"}},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":"# **Construct Sampling Table**","metadata":{}},{"cell_type":"code","source":"def create_sampling_table(table_size, power, idx_to_word, word_counts, vocab_size):        \n    sampling_table = np.zeros(int(table_size), dtype = np.uint32)\n    norm = sum(word_counts[idx_to_word[i]]** power for i in range(vocab_size))\n\n    p = 0 \n    i = 0\n    \n    for j in range(vocab_size):\n        word = idx_to_word[j]\n        count = word_counts[word]\n\n        p += (count ** power) / norm\n\n        while i < table_size and (i / table_size) < p:\n            sampling_table[i] = j\n            i += 1\n    \n    return sampling_table","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-18T10:30:25.572705Z","iopub.execute_input":"2025-11-18T10:30:25.573478Z","iopub.status.idle":"2025-11-18T10:30:25.590914Z","shell.execute_reply.started":"2025-11-18T10:30:25.573428Z","shell.execute_reply":"2025-11-18T10:30:25.589991Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"sampling_table = create_sampling_table(1e8, .75, idx_to_word, word_counts, vocab_size)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-18T10:30:25.592071Z","iopub.execute_input":"2025-11-18T10:30:25.592424Z","iopub.status.idle":"2025-11-18T10:30:47.457659Z","shell.execute_reply.started":"2025-11-18T10:30:25.592382Z","shell.execute_reply":"2025-11-18T10:30:47.456433Z"}},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":"# **Weights Intialization**","metadata":{}},{"cell_type":"code","source":"def initialize_weights(vocab_size, embed_dim):\n\n    target_vectors = (np.random.rand(vocab_size, embed_dim) - 0.5) / embed_dim\n    context_vectors = np.zeros((vocab_size, embed_dim))\n\n    return target_vectors, context_vectors","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-18T10:30:47.458723Z","iopub.execute_input":"2025-11-18T10:30:47.458969Z","iopub.status.idle":"2025-11-18T10:30:47.464075Z","shell.execute_reply.started":"2025-11-18T10:30:47.458952Z","shell.execute_reply":"2025-11-18T10:30:47.463202Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"target_vectors, context_vectors = initialize_weights(vocab_size, embed_dim)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-18T10:30:47.46479Z","iopub.execute_input":"2025-11-18T10:30:47.465077Z","iopub.status.idle":"2025-11-18T10:30:47.531573Z","shell.execute_reply.started":"2025-11-18T10:30:47.465056Z","shell.execute_reply":"2025-11-18T10:30:47.530353Z"}},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":"# **Extract Negative Samples**","metadata":{}},{"cell_type":"code","source":"def get_negative_samples(target_idx, num_negative_samples, sampling_table):\n    samples = []\n    \n    while len(samples) < num_negative_samples:\n        sample_idx = sampling_table[random.randint(0, len(sampling_table) - 1)]\n        if sample_idx != target_idx:\n            samples.append(sample_idx)\n    \n    return samples","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-18T10:30:47.532695Z","iopub.execute_input":"2025-11-18T10:30:47.533125Z","iopub.status.idle":"2025-11-18T10:30:47.539456Z","shell.execute_reply.started":"2025-11-18T10:30:47.533092Z","shell.execute_reply":"2025-11-18T10:30:47.538388Z"}},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":"# **Training Function**","metadata":{}},{"cell_type":"code","source":"def train(corpus, epochs, start_alpha, min_alpha, num_negative_samples, sampling_table, word_to_idx, context_size, embed_dim, target_vectors, context_vectors):\n    \n    total_steps = epochs * len(corpus)\n    current_step = 0\n    epoch_losses = []\n\n    for epoch in range(1, epochs + 1):\n        print(f\"\\n--- Epoch {epoch}/{epochs} ---\")\n        epoch_loss = 0\n\n        for sentence in tqdm(corpus, desc=f\"Epoch {epoch}\"):\n\n            progress = current_step / total_steps\n            alpha = start_alpha * (1.0 - progress)\n            alpha = max(min_alpha, alpha)\n            current_step += 1\n\n            sentence_indices = [word_to_idx[word] for word in sentence if word in word_to_idx]\n\n            if len(sentence_indices) < 2:\n                continue\n\n            for i, target_idx in enumerate(sentence_indices):\n                current_window = random.randint(1, context_size)\n                start = max(0, i - current_window)\n                end = min(len(sentence_indices), i + current_window + 1)\n                context_indices = [sentence_indices[j] for j in range(start, end) if i != j]\n\n                if not context_indices:\n                    continue\n\n                grad_target = np.zeros(embed_dim)\n\n                for context_idx in context_indices:\n                    score = target_vectors[target_idx].dot(context_vectors[context_idx])\n                    prob = sigmoid(score)\n                    gradient = alpha * (1 - prob)\n                    grad_target += gradient * context_vectors[context_idx]\n                    grad_context = gradient * target_vectors[target_idx]\n                    context_vectors[context_idx] += grad_context\n                    epoch_loss += -np.log(prob + 1e-10)\n\n                negative_samples = get_negative_samples(target_idx, num_negative_samples, sampling_table)\n\n                for neg_idx in negative_samples:\n                    score = target_vectors[target_idx].dot(context_vectors[neg_idx])\n                    prob = sigmoid(score)\n                    gradient = alpha * (0 - prob)\n                    grad_target += gradient * context_vectors[neg_idx]\n                    grad_context = gradient * target_vectors[target_idx]\n                    context_vectors[neg_idx] += grad_context\n                    epoch_loss += -np.log(1 - prob + 1e-10)\n\n                target_vectors[target_idx] += grad_target\n\n        print(epoch_loss)\n        epoch_losses.append(epoch_loss)\n\n    print(\"\\nTraining complete.\")\n    return target_vectors, epoch_losses","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-18T10:38:52.303317Z","iopub.execute_input":"2025-11-18T10:38:52.303676Z","iopub.status.idle":"2025-11-18T10:38:52.336735Z","shell.execute_reply.started":"2025-11-18T10:38:52.303654Z","shell.execute_reply":"2025-11-18T10:38:52.335459Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"target_vector, epoch_losses = train(corpus, epochs, start_alpha, min_alpha, num_negative_samples, sampling_table, word_to_idx, context_size, embed_dim, target_vectors, context_vectors)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-18T10:38:52.989015Z","iopub.execute_input":"2025-11-18T10:38:52.989684Z"}},"outputs":[{"name":"stdout","text":"\n--- Epoch 1/100 ---\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1:  61%|██████    | 12589/20744 [00:34<00:17, 469.48it/s]","output_type":"stream"}],"execution_count":null},{"cell_type":"markdown","source":"# **Plot Loesses**","metadata":{}},{"cell_type":"code","source":"def plot_epoch_losses(epoch_losses, save_path = \"epoch_losses.png\"):\n    plt.figure(figsize = (8, 5))\n    plt.plot(range(1, len(epoch_losses) + 1), epoch_losses, marker = 'o', color = 'skyblue')\n    plt.title(\"Training Loss per Epoch\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.xticks(range(1, len(epoch_losses) + 1))\n    plt.grid(True, linestyle = '--', alpha = 0.5)\n    plt.savefig(save_path, bbox_inches = 'tight')\n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-18T10:38:22.572107Z","iopub.execute_input":"2025-11-18T10:38:22.572506Z","iopub.status.idle":"2025-11-18T10:38:22.579095Z","shell.execute_reply.started":"2025-11-18T10:38:22.572484Z","shell.execute_reply":"2025-11-18T10:38:22.578153Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"plot_epoch_losses(epoch_losses)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-18T10:38:26.051778Z","iopub.execute_input":"2025-11-18T10:38:26.052098Z","iopub.status.idle":"2025-11-18T10:38:26.068661Z","shell.execute_reply.started":"2025-11-18T10:38:26.052076Z","shell.execute_reply":"2025-11-18T10:38:26.06719Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_48/4027055584.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplot_epoch_losses\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_losses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'epoch_losses' is not defined"],"ename":"NameError","evalue":"name 'epoch_losses' is not defined","output_type":"error"}],"execution_count":27},{"cell_type":"markdown","source":"# **Save Embeddings And Word to Index Mappnig**","metadata":{}},{"cell_type":"code","source":"def save_embeddings_and_vocab(embeddings, word_to_idx, idx_to_word, embeddings_path, word_to_idx_path, idx_to_word_path):\n    np.save(embeddings_path, embeddings)\n    with open(word_to_idx_path, 'wb') as f:\n        pickle.dump(word_to_idx, f)\n    with open(idx_to_word_path, 'wb') as f:\n        pickle.dump(idx_to_word, f)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-18T10:32:49.300322Z","iopub.status.idle":"2025-11-18T10:32:49.300657Z","shell.execute_reply.started":"2025-11-18T10:32:49.300465Z","shell.execute_reply":"2025-11-18T10:32:49.300486Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"save_embeddings_and_vocab(target_vectors, word_to_idx, idx_to_word, 'embeddings.npy', 'word_to_idx.pkl', 'idx_to_word.pkl')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-18T10:32:49.302055Z","iopub.status.idle":"2025-11-18T10:32:49.30262Z","shell.execute_reply.started":"2025-11-18T10:32:49.302392Z","shell.execute_reply":"2025-11-18T10:32:49.302412Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Load Embeddings And Word to Index Mappnig**","metadata":{}},{"cell_type":"code","source":"def load_embeddings_and_vocab(embeddings_path, word_to_idx_path, idx_to_word_path):\n    embeddings = np.load(embeddings_path)\n    with open(word_to_idx_path, 'rb') as f:\n        word_to_idx = pickle.load(f)\n    with open(idx_to_word_path, 'rb') as f:\n        idx_to_word = pickle.load(f)\n    return embeddings, word_to_idx, idx_to_word ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-18T10:32:49.304495Z","iopub.status.idle":"2025-11-18T10:32:49.304917Z","shell.execute_reply.started":"2025-11-18T10:32:49.304728Z","shell.execute_reply":"2025-11-18T10:32:49.304746Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"embeddings, word_to_idx, idx_to_word = load_embeddings_and_vocab('embeddings.npy', 'word_to_idx.pkl', 'idx_to_word.pkl')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-18T10:32:49.306905Z","iopub.status.idle":"2025-11-18T10:32:49.307309Z","shell.execute_reply.started":"2025-11-18T10:32:49.307111Z","shell.execute_reply":"2025-11-18T10:32:49.307128Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Testing**","metadata":{}},{"cell_type":"code","source":"vectors_normed = embeddings / (np.linalg.norm(embeddings, axis=1, keepdims=True) + 1e-10)\n\ndef cosine_similarity(a, b):\n    return np.dot(a, b)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-18T10:32:49.308465Z","iopub.status.idle":"2025-11-18T10:32:49.308894Z","shell.execute_reply.started":"2025-11-18T10:32:49.308704Z","shell.execute_reply":"2025-11-18T10:32:49.308722Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **Similar Words**","metadata":{}},{"cell_type":"code","source":"def most_similar(word, word_to_idx, idx_to_word, vectors_normed, topn=5):\n    if word not in word_to_idx:\n        print(f\"Error: '{word}' not in vocabulary.\"); return\n\n    idx = word_to_idx[word]\n    target = vectors_normed[idx]\n    sims = np.dot(vectors_normed, target)\n    sims[idx] = -np.inf\n    top = np.argsort(sims)[-topn:][::-1]\n\n    print(\"\\n\" + \"=\"*50)\n    print(f\"Most Similar Words to '{word}':\")\n    print(\"=\"*50)\n    for i in top:\n        print(f\"  {idx_to_word[i]:<15} | Similarity: {sims[i]:.4f}\")\n    print(\"-\"*50 + \"\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-18T10:32:49.311048Z","iopub.status.idle":"2025-11-18T10:32:49.311446Z","shell.execute_reply.started":"2025-11-18T10:32:49.311249Z","shell.execute_reply":"2025-11-18T10:32:49.311266Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_words = ['france', 'germany', 'london', 'paris', 'company', 'government']\nfor w in test_words:\n    most_similar(w, word_to_idx, idx_to_word, vectors_normed)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-18T10:32:49.31294Z","iopub.status.idle":"2025-11-18T10:32:49.31357Z","shell.execute_reply.started":"2025-11-18T10:32:49.313328Z","shell.execute_reply":"2025-11-18T10:32:49.313347Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **Word Analogy**","metadata":{}},{"cell_type":"code","source":"def word_analogy_top_words(a, b, c, expected, word_to_idx, idx_to_word, vectors, topn=100):\n    for w in [a,b,c,expected]:\n        if w not in word_to_idx: \n            print(f\"Error: '{w}' not in vocabulary.\"); return\n\n    vec = vectors[word_to_idx[b]] - vectors[word_to_idx[a]] + vectors[word_to_idx[c]]\n    vec /= np.linalg.norm(vec) + 1e-10\n    sims = np.dot(vectors, vec)\n    for i in {word_to_idx[a], word_to_idx[b], word_to_idx[c]}: sims[i] = -np.inf\n\n    pred_idx = np.argmax(sims)\n    pred_word = idx_to_word[pred_idx]\n\n    print(\"\\n\" + \"=\"*70)\n    print(f\"Word Analogy Test: '{a}' : '{b}' :: '{c}' : ?\")\n    print(\"=\"*70)\n    print(f\"Predicted: {pred_word} ({sims[pred_idx]:.4f}) | Expected: {expected} ({sims[word_to_idx[expected]]:.4f})\")\n    print(\"-\"*70)\n\n    top_words_idx = np.argsort(sims)[-topn:]\n    top_words = [idx_to_word[i] for i in top_words_idx if idx_to_word[i] not in [expected,pred_word]]\n    top_vectors = np.array([vectors[word_to_idx[w]] for w in top_words])\n\n    all_vectors = np.vstack([top_vectors, vectors[word_to_idx[expected]], vec, vectors[word_to_idx[pred_word]]])\n    red = TSNE(n_components=2, random_state=42, init='pca', perplexity=30, learning_rate='auto').fit_transform(all_vectors)\n    \n    plt.figure(figsize=(20,20)); off=0.1\n    for i,w in enumerate(top_words):\n        x,y=red[i]; plt.scatter(x,y,color='gray',s=30,alpha=0.6); plt.text(x+off,y+off,w,fontsize=8)\n    \n    colors = ['green','red','blue']\n    labels = [expected,'Result (b-a+c)',pred_word]\n    for i,(x,y) in enumerate(red[len(top_words):]): \n        plt.scatter(x,y,color=colors[i],s=[80,100,80][i],label=labels[i])\n        plt.text(x+off,y+off,labels[i],fontsize=10,fontweight='bold')\n    \n    plt.title(f\"Word Analogy TSNE: '{a}' : '{b}' :: '{c}' : ?\", fontsize=14)\n    plt.xlabel(\"TSNE Dim 1\"); plt.ylabel(\"TSNE Dim 2\"); plt.axis('equal'); plt.grid(True,linestyle='--',alpha=0.5); plt.legend()\n    \n    fname=f\"analogy_{a}_{b}_{c}_{expected}.png\".replace(\" \",\"_\")\n    plt.savefig(fname,bbox_inches='tight',dpi=300); print(f\"Plot saved as {fname}\"); plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-18T10:32:49.315135Z","iopub.status.idle":"2025-11-18T10:32:49.315837Z","shell.execute_reply.started":"2025-11-18T10:32:49.315688Z","shell.execute_reply":"2025-11-18T10:32:49.315703Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"analogies = [\n    ('man', 'men', 'woman', 'women'),\n    ('big', 'bigger', 'small', 'smaller'),\n    ('good', 'better', 'bad', 'worse')\n]\n\nfor a, b, c, d in analogies:\n    word_analogy_top_words(a, b, c, d, word_to_idx, idx_to_word, vectors_normed)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-18T10:32:49.316603Z","iopub.status.idle":"2025-11-18T10:32:49.316858Z","shell.execute_reply.started":"2025-11-18T10:32:49.316742Z","shell.execute_reply":"2025-11-18T10:32:49.316752Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **Random Words Similarities**","metadata":{}},{"cell_type":"code","source":"def random_word_similarity(word_to_idx, idx_to_word, vectors_normed, num_pairs=5):\n    words = list(word_to_idx.keys())\n    print(\"\\n\" + \"=\"*50)\n    print(f\"Random Word Similarity Test ({num_pairs} Pairs)\")\n    print(\"=\"*50)\n    \n    for _ in range(num_pairs):\n        w1, w2 = random.sample(words, 2)\n        idx1, idx2 = word_to_idx[w1], word_to_idx[w2]\n        score = np.dot(vectors_normed[idx1], vectors_normed[idx2])\n        print(f\"  {w1:<15} | {w2:<15} | Similarity: {score:.4f}\")\n    \n    print(\"-\"*50 + \"\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-18T10:32:49.317879Z","iopub.status.idle":"2025-11-18T10:32:49.318367Z","shell.execute_reply.started":"2025-11-18T10:32:49.318099Z","shell.execute_reply":"2025-11-18T10:32:49.318113Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"random_word_similarity(word_to_idx, idx_to_word, vectors_normed, num_pairs = 5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-18T10:32:49.319541Z","iopub.status.idle":"2025-11-18T10:32:49.319844Z","shell.execute_reply.started":"2025-11-18T10:32:49.319702Z","shell.execute_reply":"2025-11-18T10:32:49.319717Z"}},"outputs":[],"execution_count":null}]}