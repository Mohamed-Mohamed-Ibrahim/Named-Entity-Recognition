{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from datasets import load_dataset, load_from_disk\nimport numpy as np\nfrom sklearn.preprocessing import LabelEncoder\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-15T08:48:04.833581Z","iopub.execute_input":"2025-11-15T08:48:04.833775Z","iopub.status.idle":"2025-11-15T08:48:16.558476Z","shell.execute_reply.started":"2025-11-15T08:48:04.833757Z","shell.execute_reply":"2025-11-15T08:48:16.557428Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"\nclass HMMCustom:\n    def __init__(self, n_components, n_observations, startprob=None, transmat=None, emissionprob=None, strategy=\"viterbi\"):\n\n        self.n_components_ = n_components\n        self.n_observations_ = n_observations\n        self.strategy = strategy\n\n        if startprob is None:\n            self.startprob_ = np.zeros(n_components)\n        else:\n            self.startprob_ = startprob\n        if transmat is None:\n            self.transmat_ = np.zeros((n_components, n_components))\n        else:\n            self.transmat_ = transmat\n        if emissionprob is None:\n            self.emissionprob_ = np.zeros((n_components, n_observations))\n        else:\n            self.emissionprob_ = emissionprob\n\n    def fit(self, X, y):\n\n        # Get start & transition & emission probs\n        for idx, sentence in enumerate(X):\n            for i, word in enumerate(sentence):\n\n                # Get start prob\n                if i == 0:\n                    self.startprob_[y[idx][i]] += 1\n                # Get transition prob\n                else:\n                    self.transmat_[y[idx][i], y[idx][i-1]] += 1\n\n                # Get emission prob\n                self.emissionprob_[y[idx][i], word] += 1\n\n        # Get start & transition & emission probs\n        for i in range(self.n_components_):\n            self.startprob_[i] += 1\n            for j in range(self.n_components_):\n                self.transmat_[i, j] += 1\n            for j in range(self.n_observations_):\n                self.emissionprob_[i, j] += 1\n\n        with np.errstate(divide='ignore', invalid='ignore'):\n            self.startprob_ /= np.sum(self.startprob_)\n            self.emissionprob_ /= ( np.sum(self.emissionprob_, axis=1).reshape(-1, 1) )\n            # self.emissionprob_ /= ( np.sum(self.transmat_, axis=1).reshape(-1, 1) + self.startprob_.reshape(-1, 1) )\n            self.transmat_ /= np.sum(self.transmat_, axis=1).reshape(-1, 1)\n            # self.transmat_ /= np.sum(self.transmat_, axis=1)[:, np.newaxis]\n        self.transmat_ = np.nan_to_num(self.transmat_)\n        self.emissionprob_ = np.nan_to_num(self.emissionprob_)\n\n        # print(self.startprob_)\n        # print()\n        # for x in self.transmat_:\n        #     print(x)\n        # print()\n        # for x in self.emissionprob_:\n        #     print(x)\n        # print()\n\n    def _greedy(self, X):\n        log_likelihood, hidden_states = 0, []\n        prev_state = None\n\n        for i, word in enumerate(X):\n            score = -1\n            if i == 0:\n                for state in range(self.n_components_):\n                    prob = self.startprob_[state] * self.emissionprob_[state][word]\n                    if prob > score:\n                        score = prob\n                        prev_state = state\n            else:\n                for state in range(self.n_components_):\n                    prob = self.transmat_[state][prev_state] * self.emissionprob_[state][word]\n                    if prob > score:\n                        score = prob\n                        prev_state = state\n\n            hidden_states.append(prev_state)\n\n            log_likelihood += score\n            # print(score, prev_state)\n\n        return log_likelihood, hidden_states\n\n    def _viterbi(self, X):\n        log_likelihood, hidden_states = 0, []\n\n        n_steps = len(X)\n        m = np.zeros((self.n_components_, n_steps))\n        parent = np.ones((self.n_components_, n_steps), dtype=int)\n\n        for i, word in enumerate(X):\n            if i == 0:\n                for state in range(self.n_components_):\n                    # print(self.startprob_[state], self.emissionprob_[state][word])\n                    prob = self.startprob_[state] * self.emissionprob_[state][word]\n                    if prob > m[state, i]:\n                        parent[state, i] = state\n                        m[state, i] = prob\n            else:\n                for s1 in range(self.n_components_):        # prev state\n                    for s2 in range(self.n_components_):    # cur  state\n                        # print(self.transmat_[s2, s1], self.emissionprob_[s2, word], m[s1, i-1])\n                        prob = self.transmat_[s2, s1] * self.emissionprob_[s2, word] * m[s1, i-1]\n\n                        if prob > m[s2, i]:\n                            parent[s2, i] = s1\n                            m[s2, i] = prob\n\n        # print()\n        # for x in m:\n        #     print(x)\n        # print()\n        # for x in parent:\n        #     print(x)\n\n        mostLikelyStateIdx = np.argmax(m[:, -1])\n        hidden_states.append(mostLikelyStateIdx)\n        log_likelihood += m[mostLikelyStateIdx, -1]\n        i = n_steps - 2\n\n        while i > 0:\n            # put parent of state i\n            hidden_states.append(parent[mostLikelyStateIdx, i+1])\n            # add likelihood of state i\n            log_likelihood += m[parent[mostLikelyStateIdx, i+1], i]\n            mostLikelyStateIdx = hidden_states[-1]\n            i -= 1\n\n        hidden_states.append(parent[mostLikelyStateIdx, 1])\n        log_likelihood += m[hidden_states[-1], 0]\n\n\n        return log_likelihood, reversed(hidden_states)\n\n    def decode(self, X):\n\n        if self.strategy == \"viterbi\":\n            return self._viterbi(X)\n        elif self.strategy == \"greedy\":\n            return self._greedy(X)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-15T08:48:16.560006Z","iopub.execute_input":"2025-11-15T08:48:16.560660Z","iopub.status.idle":"2025-11-15T08:48:16.579039Z","shell.execute_reply.started":"2025-11-15T08:48:16.560626Z","shell.execute_reply":"2025-11-15T08:48:16.577966Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"dataset = load_dataset(\"lhoestq/conll2003\")\ndataset.save_to_disk(\"conll2003\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-15T08:48:16.580274Z","iopub.execute_input":"2025-11-15T08:48:16.580727Z","iopub.status.idle":"2025-11-15T08:48:19.330213Z","shell.execute_reply.started":"2025-11-15T08:48:16.580699Z","shell.execute_reply":"2025-11-15T08:48:19.329258Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"dataset_infos.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"41f03b71d5c44ef197f546bd69bb8e10"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"data/train-00000-of-00001.parquet:   0%|          | 0.00/1.07M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"701730ed00d04254a29987dc2852cf10"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"data/validation-00000-of-00001.parquet:   0%|          | 0.00/281k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2fbe1da5306b4dc7abf4e6d523da572d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"data/test-00000-of-00001.parquet:   0%|          | 0.00/259k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e82b98d178934e2f8f5d0d962b5fc07a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/14041 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"66debc2af2c443089e774555ba06b500"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/3250 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"17d6b686ca8b4ee6a96f2ce3cfcc8735"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/3453 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2a0ad18bf2074d1a85af3ff8f5db82c5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/1 shards):   0%|          | 0/14041 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c0f066872ea140da924b95333414dfb7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/1 shards):   0%|          | 0/3250 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1f73103ce319489289ff80068d03687e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/1 shards):   0%|          | 0/3453 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aad7819d156c499c8b28bcd53778e6e0"}},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-15T08:48:19.332715Z","iopub.execute_input":"2025-11-15T08:48:19.333010Z","iopub.status.idle":"2025-11-15T08:48:19.339628Z","shell.execute_reply.started":"2025-11-15T08:48:19.332986Z","shell.execute_reply":"2025-11-15T08:48:19.338654Z"}},"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n        num_rows: 14041\n    })\n    validation: Dataset({\n        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n        num_rows: 3250\n    })\n    test: Dataset({\n        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n        num_rows: 3453\n    })\n})"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"random_state = 42\n\nnerTags = dataset[\"train\"][:]['ner_tags']\ntokens = dataset[\"train\"][:]['tokens']\nstates = [\"Other\", \"B-PER\", \"I-PER\", \"B-ORG\", \"I-ORG\", \"B-LOC\", \"I-LOC\", \"B-MISC\", \"I-MISC\"]\nobservations = set()\nmaxLen = 0\nobservations.add(\" \")\nfor token in tokens:\n    maxLen = max(maxLen, len(token))\n    for word in token:\n        observations.add(word.lower())\nobservations = list(sorted(observations))\n# print(\"Observations:\", observations)\n\nle = LabelEncoder()\nencoded_data = le.fit_transform(observations)\n# print(\"Encoded data:\", encoded_data)\n\nX = []\nfor token in tokens:\n    encoded_tokens = le.transform([word.lower() for word in token])\n    X.append(encoded_tokens.tolist())\n\n# print(X)\n# print(tokens)\n# print(nerTags)\n\nn_components = len(states)\nn_observations = len(observations)\n# print(n_components)\n# print(n_observations)\n\nmodel = HMMCustom(n_components=n_components, n_observations=n_observations)\n\nmodel.fit(X, nerTags)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-15T08:48:19.340822Z","iopub.execute_input":"2025-11-15T08:48:19.341174Z","iopub.status.idle":"2025-11-15T08:53:01.677066Z","shell.execute_reply.started":"2025-11-15T08:48:19.341138Z","shell.execute_reply":"2025-11-15T08:53:01.675795Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"n_samples = 5\n\n\ny_val = dataset[\"validation\"][40:45]['ner_tags']\nX_val_tmp = dataset[\"validation\"][40:45]['tokens']\n\n# print(\"y val:\", y_val)\n\nX_val = []\ndropped_words = {}\nfor i, token in enumerate(X_val_tmp):\n    lowercaseToken = []\n    for j, word in enumerate(token):\n        word = word.lower()\n        if word in le.classes_:\n            lowercaseToken.append(word)\n        else:\n            # print(dropped_words)\n            if i not in dropped_words:\n                dropped_words[i] = [(j, word)]\n            else:\n                dropped_words[i].append((j, word))\n    encoded_tokens = le.transform(lowercaseToken)\n    X_val.append(encoded_tokens.tolist())\n    \n# print(\"X val:\", X_val)\n# print(\"Dropped words:\")\nfor key, val in dropped_words.items():\n    for i, x in enumerate(val):\n        print(f\"token {key+1}, word {i+1}: {x}\")\nprint()\n\ny_test = []\nfor token in y_val:\n    y_test.extend(token)\ny_pred = []\n\nfor i, sample in enumerate(X_val):\n    log_likelihood, hidden_states = model.decode(sample)\n    hidden_states = list(hidden_states)\n    l = len(hidden_states)\n    n = len(y_val[i])\n    print(hidden_states)\n    if i in dropped_words:\n        for j, word in reversed(dropped_words[i]):\n            if j >= l:\n                hidden_states.append(0)\n            else:\n                print(j)\n                hidden_states.insert(j, 0)\n    print(hidden_states)\n    y_pred.extend(hidden_states)\n    print(\"Sample\", i+1)\n    print(\"Log-likelihood of observations:\", log_likelihood)\n    print(\"Most likely sequence:\", [states[s] for s in hidden_states])\n    print(\"Observation sequence:\", [states[s] for s in y_val[i]])\n    \n    print()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-15T09:21:30.036569Z","iopub.execute_input":"2025-11-15T09:21:30.036943Z","iopub.status.idle":"2025-11-15T09:21:30.181832Z","shell.execute_reply.started":"2025-11-15T09:21:30.036915Z","shell.execute_reply":"2025-11-15T09:21:30.180613Z"}},"outputs":[{"name":"stdout","text":"token 2, word 1: (1, 'itinerary')\n\n[5, 0]\n[5, 0]\nSample 1\nLog-likelihood of observations: 9.212912095240803e-05\nMost likely sequence: ['B-LOC', 'Other']\nObservation sequence: ['B-LOC', 'Other']\n\n[0, 0]\n1\n[0, 0, 0]\nSample 2\nLog-likelihood of observations: 0.00011303176755618886\nMost likely sequence: ['Other', 'Other', 'Other']\nObservation sequence: ['Other', 'Other', 'Other']\n\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_48/2530265435.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m     \u001b[0mlog_likelihood\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m     \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_48/2617661566.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrategy\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"viterbi\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_viterbi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrategy\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"greedy\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_greedy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_48/2617661566.py\u001b[0m in \u001b[0;36m_viterbi\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    132\u001b[0m             \u001b[0mi\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m         \u001b[0mhidden_states\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmostLikelyStateIdx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m         \u001b[0mlog_likelihood\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mIndexError\u001b[0m: index 1 is out of bounds for axis 1 with size 1"],"ename":"IndexError","evalue":"index 1 is out of bounds for axis 1 with size 1","output_type":"error"}],"execution_count":57},{"cell_type":"code","source":"from sklearn.metrics import precision_score, recall_score, f1_score\n# y_test = y_test[:772]\nprecision = precision_score(y_test, y_pred, average='macro')\nrecall = recall_score(y_test, y_pred, average='macro')\nf1 = f1_score(y_test, y_pred, average='macro')\n\nprint(f\"Precision: {precision:.4f}\")\nprint(f\"Recall: {recall:.4f}\")\nprint(f\"F1-score: {f1:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-15T09:18:53.766572Z","iopub.execute_input":"2025-11-15T09:18:53.766884Z","iopub.status.idle":"2025-11-15T09:18:53.783173Z","shell.execute_reply.started":"2025-11-15T09:18:53.766863Z","shell.execute_reply":"2025-11-15T09:18:53.782126Z"}},"outputs":[{"name":"stdout","text":"Precision: 0.5998\nRecall: 0.4964\nF1-score: 0.5359\n","output_type":"stream"}],"execution_count":55},{"cell_type":"code","source":"n_samples = 5\n\n\ny_val = dataset[\"test\"][:]['ner_tags']\nX_val_tmp = dataset[\"test\"][:]['tokens']\n\n# print(\"y val:\", y_val)\n\nX_val = []\ndropped_words = {}\nfor i, token in enumerate(X_val_tmp):\n    lowercaseToken = []\n    for j, word in enumerate(token):\n        word = word.lower()\n        if word in le.classes_:\n            lowercaseToken.append(word)\n        else:\n            # print(dropped_words)\n            if i not in dropped_words:\n                dropped_words[i] = [(j, word)]\n            else:\n                dropped_words[i].append((j, word))\n    encoded_tokens = le.transform(lowercaseToken)\n    X_val.append(encoded_tokens.tolist())\n    \n# print(\"X val:\", X_val)\nprint(\"Dropped words:\")\nfor key, val in dropped_words.items():\n    for i, x in enumerate(val):\n        print(f\"token {key+1}, word {i+1}: {x}\")\nprint()\n\ny_test = []\nfor token in y_val:\n    y_test.extend(token)\ny_pred = []\n\nfor i, sample in enumerate(X_val):\n    log_likelihood, hidden_states = model.decode(sample)\n    hidden_states = list(hidden_states)\n    l = len(hidden_states)\n    n = len(y_val[i])\n    if i in dropped_words:\n        for j, word in reversed(dropped_words[i]):\n            if j >= l:\n                hidden_states.append(0)\n            else:\n                print(j)\n                hidden_states.insert(j, 0)\n    y_pred.extend(hidden_states)\n    print(\"Sample\", i+1)\n    print(\"Log-likelihood of observations:\", log_likelihood)\n    print(\"Most likely sequence:\", [states[s] for s in hidden_states])\n    print(\"Observation sequence:\", [states[s] for s in y_val[i]])\n    \n    print()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-15T09:16:45.253886Z","iopub.status.idle":"2025-11-15T09:16:45.254385Z","shell.execute_reply.started":"2025-11-15T09:16:45.254059Z","shell.execute_reply":"2025-11-15T09:16:45.254100Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import precision_score, recall_score, f1_score\n\nprecision = precision_score(y_test, y_pred, average='macro')\nrecall = recall_score(y_test, y_pred, average='macro')\nf1 = f1_score(y_test, y_pred, average='macro')\n\nprint(f\"Precision: {precision:.4f}\")\nprint(f\"Recall: {recall:.4f}\")\nprint(f\"F1-score: {f1:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-15T09:16:45.256797Z","iopub.status.idle":"2025-11-15T09:16:45.257255Z","shell.execute_reply.started":"2025-11-15T09:16:45.257031Z","shell.execute_reply":"2025-11-15T09:16:45.257050Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}