{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# ðŸ§¬ Named Entity Recognition using HMM & Viterbi\n\nThis notebook implements a **Hidden Markov Model (HMM)** from scratch for the task of **Named Entity Recognition (NER)** on the CoNLL-2003 dataset. \n\nWhile modern NLP often relies on Transformers, this project explores the mathematical efficiency of probabilistic graphical models. By calculating **Transition** and **Emission** probabilities, the model learns the underlying structure of entity sequences (e.g., how a `B-PER` tag is statistically likely to be followed by an `I-PER` tag).\n\n### ðŸš€ Key Features:\n* **Supervised Training:** Computing probability matrices from the CoNLL-2003 training split.\n* **Viterbi Algorithm:** Implementation of the dynamic programming approach to decode the most likely sequence of hidden states.\n* **Evaluation:** Detailed classification report covering Precision, Recall, and F1-Score across 9 entity classes.\n* **Statistical Insights:** Analysis of how probabilistic models handle unseen words and state transitions.\n\n### ðŸ”— Source Code & Full Project\nThe complete implementation, including the Word2Vec embedding training and neural network comparisons, can be found on GitHub:\n\nðŸ‘‰ [**View Project on GitHub**](https://github.com/Mohamed-Mohamed-Ibrahim/Named-Entity-Recognition)","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset, load_from_disk\nimport numpy as np\nfrom sklearn.preprocessing import LabelEncoder\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-19T06:11:05.778208Z","iopub.execute_input":"2025-11-19T06:11:05.778919Z","iopub.status.idle":"2025-11-19T06:11:15.995140Z","shell.execute_reply.started":"2025-11-19T06:11:05.778888Z","shell.execute_reply":"2025-11-19T06:11:15.994357Z"}},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"# Model","metadata":{}},{"cell_type":"code","source":"\nclass HMMCustom:\n    def __init__(self, n_components, n_observations, startprob=None, transmat=None, emissionprob=None, strategy=\"viterbi\"):\n\n        self.n_components_ = n_components\n        self.n_observations_ = n_observations\n        self.strategy = strategy\n\n        if startprob is None:\n            self.startprob_ = np.zeros(n_components)\n        else:\n            self.startprob_ = startprob\n        if transmat is None:\n            self.transmat_ = np.zeros((n_components, n_components))\n        else:\n            self.transmat_ = transmat\n        if emissionprob is None:\n            self.emissionprob_ = np.zeros((n_components, n_observations))\n        else:\n            self.emissionprob_ = emissionprob\n\n    def fit(self, X, y):\n\n        # Get start & transition & emission probs\n        for idx, sentence in enumerate(X):\n            for i, word in enumerate(sentence):\n\n                # Get start prob\n                if i == 0:\n                    self.startprob_[y[idx][i]] += 1\n                # Get transition prob\n                else:\n                    self.transmat_[y[idx][i], y[idx][i-1]] += 1\n\n                # Get emission prob\n                self.emissionprob_[y[idx][i], word] += 1\n\n        # Get start & transition & emission probs\n        for i in range(self.n_components_):\n            self.startprob_[i] += 1\n            for j in range(self.n_components_):\n                self.transmat_[i, j] += 1\n            for j in range(self.n_observations_):\n                self.emissionprob_[i, j] += 1\n\n        with np.errstate(divide='ignore', invalid='ignore'):\n            self.startprob_ /= np.sum(self.startprob_)\n            self.emissionprob_ /= ( np.sum(self.emissionprob_, axis=1).reshape(-1, 1) )\n            # self.emissionprob_ /= ( np.sum(self.transmat_, axis=1).reshape(-1, 1) + self.startprob_.reshape(-1, 1) )\n            self.transmat_ /= np.sum(self.transmat_, axis=1).reshape(-1, 1)\n            # self.transmat_ /= np.sum(self.transmat_, axis=1)[:, np.newaxis]\n        self.transmat_ = np.nan_to_num(self.transmat_)\n        self.emissionprob_ = np.nan_to_num(self.emissionprob_)\n\n        # print(self.startprob_)\n        # print()\n        # for x in self.transmat_:\n        #     print(x)\n        # print()\n        # for x in self.emissionprob_:\n        #     print(x)\n        # print()\n\n    def _greedy(self, X):\n        log_likelihood, hidden_states = 0, []\n        prev_state = None\n\n        for i, word in enumerate(X):\n            score = -1\n            if i == 0:\n                for state in range(self.n_components_):\n                    prob = self.startprob_[state] * self.emissionprob_[state][word]\n                    if prob > score:\n                        score = prob\n                        prev_state = state\n            else:\n                for state in range(self.n_components_):\n                    prob = self.transmat_[state][prev_state] * self.emissionprob_[state][word]\n                    if prob > score:\n                        score = prob\n                        prev_state = state\n\n            hidden_states.append(prev_state)\n\n            log_likelihood += score\n            # print(score, prev_state)\n\n        return log_likelihood, hidden_states\n\n    def _viterbi(self, X):\n        log_likelihood, hidden_states = 0, []\n\n        n_steps = len(X)\n\n        if n_steps == 0:\n            return log_likelihood, hidden_states\n        \n        m = np.zeros((self.n_components_, n_steps))\n        parent = np.ones((self.n_components_, n_steps), dtype=int)\n\n        for i, word in enumerate(X):\n            if i == 0:\n                for state in range(self.n_components_):\n                    # print(self.startprob_[state], self.emissionprob_[state][word])\n                    prob = self.startprob_[state] * self.emissionprob_[state][word]\n                    if prob > m[state, i]:\n                        parent[state, i] = state\n                        m[state, i] = prob\n            else:\n                for s1 in range(self.n_components_):        # prev state\n                    for s2 in range(self.n_components_):    # cur  state\n                        # print(self.transmat_[s2, s1], self.emissionprob_[s2, word], m[s1, i-1])\n                        prob = self.transmat_[s2, s1] * self.emissionprob_[s2, word] * m[s1, i-1]\n\n                        if prob > m[s2, i]:\n                            parent[s2, i] = s1\n                            m[s2, i] = prob\n\n        # print()\n        # for x in m:\n        #     print(x)\n        # print()\n        # for x in parent:\n        #     print(x)\n\n        mostLikelyStateIdx = np.argmax(m[:, -1])\n        hidden_states.append(mostLikelyStateIdx)\n        log_likelihood += m[mostLikelyStateIdx, -1]\n        i = n_steps - 2\n\n        if n_steps > 2:\n            while i > 0:\n                # put parent of state i\n                hidden_states.append(parent[mostLikelyStateIdx, i+1])\n                # add likelihood of state i\n                log_likelihood += m[parent[mostLikelyStateIdx, i+1], i]\n                mostLikelyStateIdx = hidden_states[-1]\n                i -= 1\n        if n_steps > 1:\n            hidden_states.append(parent[mostLikelyStateIdx, 1])\n            log_likelihood += m[hidden_states[-1], 0]\n\n\n\n        return log_likelihood, reversed(hidden_states)\n\n    def decode(self, X):\n\n        if self.strategy == \"viterbi\":\n            return self._viterbi(X)\n        elif self.strategy == \"greedy\":\n            return self._greedy(X)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-19T06:11:15.996506Z","iopub.execute_input":"2025-11-19T06:11:15.996927Z","iopub.status.idle":"2025-11-19T06:11:16.014283Z","shell.execute_reply.started":"2025-11-19T06:11:15.996907Z","shell.execute_reply":"2025-11-19T06:11:16.013248Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"## Final Version","metadata":{}},{"cell_type":"code","source":"import numpy as np\n\nclass HMMCustom:\n    def __init__(self, n_components, n_observations, strategy=\"viterbi\"):\n        self.n_components_ = n_components\n        self.n_observations_ = n_observations\n        self.strategy = strategy\n\n        self.startprob_ = np.zeros(n_components)\n        self.transmat_ = np.zeros((n_components, n_components))\n        self.emissionprob_ = np.zeros((n_components, n_observations))\n\n    def fit(self, X, y):\n        # X: List of lists (sentences of observations)\n        # y: List of lists (corresponding hidden states)\n        \n        # 1. Count occurrences\n        for idx, sentence in enumerate(X):\n            for i, word in enumerate(sentence):\n                state = y[idx][i]\n                \n                # Emission: State -> Word\n                self.emissionprob_[state, word] += 1\n\n                if i == 0:\n                    # Start Probability\n                    self.startprob_[state] += 1\n                else:\n                    # Transition: Previous State -> Current State\n                    prev_state = y[idx][i-1]\n                    self.transmat_[prev_state, state] += 1 \n\n        # 2. Add Laplace Smoothing (add 1) to avoid zero division\n        self.startprob_ += 1\n        self.transmat_ += 1\n        self.emissionprob_ += 1\n\n        # 3. Normalize (probabilities sum to 1)\n        self.startprob_ /= np.sum(self.startprob_)\n        \n        # Normalize rows (axis 1) so sum of outgoing probs = 1\n        self.transmat_ /= np.sum(self.transmat_, axis=1, keepdims=True)\n        self.emissionprob_ /= np.sum(self.emissionprob_, axis=1, keepdims=True)\n\n        # 4. CONVERT TO LOG SPACE \n        # We store logs so we can add them instead of multiplying\n        with np.errstate(divide='ignore'):\n            self.startprob_ = np.log(self.startprob_)\n            self.transmat_ = np.log(self.transmat_)\n            self.emissionprob_ = np.log(self.emissionprob_)\n\n    def _viterbi(self, X):\n        n_steps = len(X)\n        if n_steps == 0:\n            return -np.inf, []\n\n        # m stores the max log-probability reaching state s at time t\n        m = np.zeros((self.n_components_, n_steps))\n        # parent stores the best previous state\n        parent = np.zeros((self.n_components_, n_steps), dtype=int)\n\n        # --- Initialization (Step 0) ---\n        for s in range(self.n_components_):\n            # log(start) + log(emission)\n            m[s, 0] = self.startprob_[s] + self.emissionprob_[s, X[0]]\n\n        # --- Recursion (Forward Step) ---\n        for t in range(1, n_steps):\n            for s in range(self.n_components_): # Current state\n                \n                # Calculate transition from all previous states (s_prev) to current state (s)\n                # vector operation: m[:, t-1] is all prev path probs\n                # transmat_[:, s] is prob of moving from any prev -> s\n                probs = m[:, t-1] + self.transmat_[:, s] + self.emissionprob_[s, X[t]]\n                \n                # Find max probability and the state that produced it\n                parent[s, t] = np.argmax(probs)\n                m[s, t] = np.max(probs)\n\n        # --- Termination ---\n        best_path_log_prob = np.max(m[:, -1])\n        last_state = np.argmax(m[:, -1])\n\n        # --- Backtracking (Backward Step) ---\n        best_path = [last_state]\n        \n        # Loop backwards from last step down to 1\n        for t in range(n_steps - 1, 0, -1):\n            prev_state = parent[best_path[-1], t]\n            best_path.append(prev_state)\n\n        # Reverse to get correct order\n        return best_path_log_prob, list(reversed(best_path))\n\n    def decode(self, X):\n        if self.strategy == \"viterbi\":\n            return self._viterbi(X)\n        return None","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-19T06:36:02.003143Z","iopub.execute_input":"2025-11-19T06:36:02.003404Z","iopub.status.idle":"2025-11-19T06:36:02.015406Z","shell.execute_reply.started":"2025-11-19T06:36:02.003386Z","shell.execute_reply":"2025-11-19T06:36:02.014600Z"}},"outputs":[],"execution_count":31},{"cell_type":"markdown","source":"# Data preparation","metadata":{}},{"cell_type":"code","source":"dataset = load_dataset(\"lhoestq/conll2003\")\ndataset.save_to_disk(\"conll2003\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-19T06:11:16.015088Z","iopub.execute_input":"2025-11-19T06:11:16.015408Z","iopub.status.idle":"2025-11-19T06:11:20.716615Z","shell.execute_reply.started":"2025-11-19T06:11:16.015379Z","shell.execute_reply":"2025-11-19T06:11:20.715923Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"dataset_infos.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"108900411a224876a5bcdce6db1e7051"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"data/train-00000-of-00001.parquet:   0%|          | 0.00/1.07M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d3f7d2202ce34192addbe2c988b0ccb3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"data/validation-00000-of-00001.parquet:   0%|          | 0.00/281k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"71ed7d6db0294fe59d45721b5aed160a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"data/test-00000-of-00001.parquet:   0%|          | 0.00/259k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"feefedbe1e0b4b65b7088ca1d4081e4d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/14041 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6a92c507eda842d68ad700a7ab7555d9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/3250 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"243c0937f1294deb8696ae48a43204c9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/3453 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2a9d81bb91344d46948883bebcbe7f90"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/1 shards):   0%|          | 0/14041 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"848acc754bf34ff8bcfccdb09702af6d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/1 shards):   0%|          | 0/3250 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b5b27c5b105640e99121795c1b9066f4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/1 shards):   0%|          | 0/3453 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f3693d0919fe47729b68e5584b833f64"}},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-19T06:11:20.718042Z","iopub.execute_input":"2025-11-19T06:11:20.718301Z","iopub.status.idle":"2025-11-19T06:11:20.724573Z","shell.execute_reply.started":"2025-11-19T06:11:20.718282Z","shell.execute_reply":"2025-11-19T06:11:20.723722Z"}},"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n        num_rows: 14041\n    })\n    validation: Dataset({\n        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n        num_rows: 3250\n    })\n    test: Dataset({\n        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n        num_rows: 3453\n    })\n})"},"metadata":{}}],"execution_count":4},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"code","source":"random_state = 42\n\n# nerTags = dataset[\"train\"][:100]['ner_tags']\nnerTags = dataset[\"train\"][:]['ner_tags']\n# tokens = dataset[\"train\"][:100]['tokens']\ntokens = dataset[\"train\"][:]['tokens']\nstates = [\"Other\", \"B-PER\", \"I-PER\", \"B-ORG\", \"I-ORG\", \"B-LOC\", \"I-LOC\", \"B-MISC\", \"I-MISC\", \"UNK\"]\nobservations = set()\nmaxLen = 0\nobservations.add(\" \")\nfor token in tokens:\n    maxLen = max(maxLen, len(token))\n    for word in token:\n        observations.add(word.lower())\nobservations = list(sorted(observations))\n# print(\"Observations:\", observations)\n\nle = LabelEncoder()\nencoded_data = le.fit_transform(observations)\n# print(\"Encoded data:\", encoded_data)\n\nX = []\nfor token in tokens:\n    encoded_tokens = le.transform([word.lower() for word in token])\n    X.append(encoded_tokens.tolist())\n\n# print(X)\n# print(tokens)\n# print(nerTags)\n\nn_components = len(states)\nn_observations = len(observations)\n# print(n_components)\n# print(n_observations)\n\nmodel = HMMCustom(n_components=n_components, n_observations=n_observations)\n\nmodel.fit(X, nerTags)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-19T06:36:04.615328Z","iopub.execute_input":"2025-11-19T06:36:04.615581Z","iopub.status.idle":"2025-11-19T06:39:46.457838Z","shell.execute_reply.started":"2025-11-19T06:36:04.615563Z","shell.execute_reply":"2025-11-19T06:39:46.457181Z"}},"outputs":[],"execution_count":32},{"cell_type":"code","source":"def insert_zeros_at_indices_numpy(original_list, excluded_indices):\n    arr = np.array(original_list)\n    excluded_indices = np.array(excluded_indices)\n    # print(arr)\n    # print(excluded_indices)\n    new_size = len(excluded_indices) + len(arr)\n    mask = np.ones(new_size, dtype=bool)\n    mask[excluded_indices] = False\n    # why [0] -> (array([3, 4]),)\n    desired_indices = np.where(mask)[0]\n    \n    output = np.zeros(new_size, dtype=np.int8) + (n_components - 1)\n    \n    output[desired_indices] = arr\n    # print(output)\n    # print()\n    return output.tolist()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-19T07:23:44.230420Z","iopub.execute_input":"2025-11-19T07:23:44.230757Z","iopub.status.idle":"2025-11-19T07:23:44.240669Z","shell.execute_reply.started":"2025-11-19T07:23:44.230730Z","shell.execute_reply":"2025-11-19T07:23:44.239519Z"}},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"# Validation","metadata":{}},{"cell_type":"code","source":"n_samples = 10\n\n\n# y_val = dataset[\"validation\"][:n_samples]['ner_tags']\ny_val = dataset[\"validation\"][:]['ner_tags']\n# X_val_tmp = dataset[\"validation\"][:n_samples]['tokens']\nX_val_tmp = dataset[\"validation\"][:]['tokens']\n\n# print(\"y val:\", y_val)\n\nX_val = []\ndropped_words = {}\nfor i, token in enumerate(X_val_tmp):\n    lowercaseToken = []\n    for j, word in enumerate(token):\n        word = word.lower()\n        if word in le.classes_:\n            lowercaseToken.append(word)\n        else:\n            if i not in dropped_words:\n                dropped_words[i] = [(j, word)]\n            else:\n                dropped_words[i].append((j, word))\n    encoded_tokens = le.transform(lowercaseToken)\n    X_val.append(encoded_tokens.tolist())\n    \n# print(\"X val:\", X_val)\n# print(\"Dropped words:\")\n# for key, val in dropped_words.items():\n#     for i, x in enumerate(val):\n#         print(f\"token {key+1}, word {i+1}: {x}\")\n# print()\n\ny_test = []\nfor token in y_val:\n    y_test.extend(token)\ny_pred = []\n\nfor i, sample in enumerate(X_val):\n    log_likelihood, hidden_states = model.decode(sample)\n    hidden_states = list(hidden_states)\n    # print(hidden_states)\n    indices = []\n    if i in dropped_words:\n        for val in dropped_words[i]:\n            indices.append(val[0])\n        hidden_states = insert_zeros_at_indices_numpy(hidden_states, indices)\n    # print(hidden_states)\n    y_pred.extend(hidden_states)\n    # print(\"Sample\", i+1)\n    # print(\"Log-likelihood of observations:\", log_likelihood)\n    # print(\"Most likely sequence:\", [states[s] for s in hidden_states])\n    # print(\"Observation sequence:\", [states[s] for s in y_val[i]])\n    \n    # print()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-19T07:23:44.541133Z","iopub.execute_input":"2025-11-19T07:23:44.541581Z","iopub.status.idle":"2025-11-19T07:23:44.619244Z","shell.execute_reply.started":"2025-11-19T07:23:44.541551Z","shell.execute_reply":"2025-11-19T07:23:44.617863Z"},"_kg_hide-input":false,"_kg_hide-output":false},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_48/3769182638.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# y_val = dataset[\"validation\"][:n_samples]['ner_tags']\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0my_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"validation\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ner_tags'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;31m# X_val_tmp = dataset[\"validation\"][:n_samples]['tokens']\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mX_val_tmp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"validation\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tokens'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'dataset' is not defined"],"ename":"NameError","evalue":"name 'dataset' is not defined","output_type":"error"}],"execution_count":2},{"cell_type":"code","source":"from sklearn.metrics import precision_score, recall_score, f1_score\n# y_test = y_test[:772]\nprecision = precision_score(y_test, y_pred, average='macro')\nrecall = recall_score(y_test, y_pred, average='macro')\nf1 = f1_score(y_test, y_pred, average='macro')\n\nprint(f\"Precision: {precision:.8f}\")\nprint(f\"Recall: {recall:.8f}\")\nprint(f\"F1-score: {f1:.8f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-19T06:40:46.296426Z","iopub.execute_input":"2025-11-19T06:40:46.297084Z","iopub.status.idle":"2025-11-19T06:40:46.448444Z","shell.execute_reply.started":"2025-11-19T06:40:46.297050Z","shell.execute_reply":"2025-11-19T06:40:46.447498Z"}},"outputs":[{"name":"stdout","text":"Precision: 0.75570854\nRecall: 0.49246153\nF1-score: 0.58535537\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"}],"execution_count":35},{"cell_type":"code","source":"from sklearn.metrics import classification_report\n\nprint(classification_report(y_test, y_pred, digits=4))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-19T06:40:46.449092Z","iopub.execute_input":"2025-11-19T06:40:46.449303Z","iopub.status.idle":"2025-11-19T06:40:46.534665Z","shell.execute_reply.started":"2025-11-19T06:40:46.449285Z","shell.execute_reply":"2025-11-19T06:40:46.533794Z"}},"outputs":[{"name":"stdout","text":"              precision    recall  f1-score   support\n\n           0     0.9581    0.9415    0.9497     42759\n           1     0.9377    0.4902    0.6439      1842\n           2     0.8717    0.5096    0.6432      1307\n           3     0.7500    0.5235    0.6166      1341\n           4     0.7343    0.3276    0.4530       751\n           5     0.8881    0.7692    0.8244      1837\n           6     0.7844    0.5097    0.6179       257\n           7     0.9350    0.5933    0.7259       922\n           8     0.6977    0.2601    0.3789       346\n           9     0.0000    0.0000    0.0000         0\n\n    accuracy                         0.8752     51362\n   macro avg     0.7557    0.4925    0.5854     51362\nweighted avg     0.9409    0.8752    0.9010     51362\n\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"}],"execution_count":36},{"cell_type":"markdown","source":"# Testing","metadata":{}},{"cell_type":"code","source":"n_samples = 5\n\n\ny_val = dataset[\"test\"][:]['ner_tags']\nX_val_tmp = dataset[\"test\"][:]['tokens']\n\n# print(\"y val:\", y_val)\n\nX_val = []\ndropped_words = {}\nfor i, token in enumerate(X_val_tmp):\n    lowercaseToken = []\n    for j, word in enumerate(token):\n        word = word.lower()\n        if word in le.classes_:\n            lowercaseToken.append(word)\n        else:\n            if i not in dropped_words:\n                dropped_words[i] = [(j, word)]\n            else:\n                dropped_words[i].append((j, word))\n    encoded_tokens = le.transform(lowercaseToken)\n    X_val.append(encoded_tokens.tolist())\n    \n# print(\"X val:\", X_val)\n# print(\"Dropped words:\")\n# for key, val in dropped_words.items():\n#     for i, x in enumerate(val):\n#         print(f\"token {key+1}, word {i+1}: {x}\")\n# print()\n\ny_test = []\nfor token in y_val:\n    y_test.extend(token)\ny_pred = []\n\nfor i, sample in enumerate(X_val):\n    log_likelihood, hidden_states = model.decode(sample)\n    hidden_states = list(hidden_states)\n    # print(hidden_states)\n    indices = []\n    if i in dropped_words:\n        for val in dropped_words[i]:\n            indices.append(val[0])\n        hidden_states = insert_zeros_at_indices_numpy(hidden_states, indices)\n    # print(hidden_states)\n    y_pred.extend(hidden_states)\n    # print(\"Sample\", i+1)\n    # print(\"Log-likelihood of observations:\", log_likelihood)\n    # print(\"Most likely sequence:\", [states[s] for s in hidden_states])\n    # print(\"Observation sequence:\", [states[s] for s in y_val[i]])\n    \n    # print()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-19T06:25:07.957765Z","iopub.execute_input":"2025-11-19T06:25:07.958117Z","iopub.status.idle":"2025-11-19T06:26:10.301995Z","shell.execute_reply.started":"2025-11-19T06:25:07.958094Z","shell.execute_reply":"2025-11-19T06:26:10.300930Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"from sklearn.metrics import precision_score, recall_score, f1_score\n\nprecision = precision_score(y_test, y_pred, average='macro')\nrecall = recall_score(y_test, y_pred, average='macro')\nf1 = f1_score(y_test, y_pred, average='macro')\n\nprint(f\"Precision: {precision:.4f}\")\nprint(f\"Recall: {recall:.4f}\")\nprint(f\"F1-score: {f1:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-19T06:26:10.302898Z","iopub.execute_input":"2025-11-19T06:26:10.303165Z","iopub.status.idle":"2025-11-19T06:26:10.442297Z","shell.execute_reply.started":"2025-11-19T06:26:10.303145Z","shell.execute_reply":"2025-11-19T06:26:10.441310Z"}},"outputs":[{"name":"stdout","text":"Precision: 0.6781\nRecall: 0.4191\nF1-score: 0.4954\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"}],"execution_count":29},{"cell_type":"code","source":"from sklearn.metrics import classification_report\n\nprint(classification_report(y_test, y_pred, digits=4))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-19T06:26:17.560524Z","iopub.execute_input":"2025-11-19T06:26:17.560791Z","iopub.status.idle":"2025-11-19T06:26:17.640093Z","shell.execute_reply.started":"2025-11-19T06:26:17.560772Z","shell.execute_reply":"2025-11-19T06:26:17.639252Z"}},"outputs":[{"name":"stdout","text":"              precision    recall  f1-score   support\n\n           0     0.9470    0.9168    0.9316     38323\n           1     0.9239    0.2777    0.4270      1617\n           2     0.8602    0.2396    0.3748      1156\n           3     0.7644    0.3125    0.4436      1661\n           4     0.6769    0.3437    0.4559       835\n           5     0.8332    0.7458    0.7871      1668\n           6     0.6221    0.5253    0.5696       257\n           7     0.6471    0.4544    0.5339       702\n           8     0.5062    0.3750    0.4309       216\n           9     0.0000    0.0000    0.0000         0\n\n    accuracy                         0.8279     46435\n   macro avg     0.6781    0.4191    0.4954     46435\nweighted avg     0.9201    0.8279    0.8586     46435\n\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"}],"execution_count":30},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}