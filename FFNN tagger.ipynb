{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13770549,"sourceType":"datasetVersion","datasetId":8764208}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# IMPORTS & CONFIG","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\n\nfrom datasets import load_dataset\nimport pickle\nimport numpy as np\nfrom sklearn.metrics import classification_report\n\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Device =\", DEVICE)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-18T17:04:41.620948Z","iopub.execute_input":"2025-11-18T17:04:41.621531Z","iopub.status.idle":"2025-11-18T17:04:41.712871Z","shell.execute_reply.started":"2025-11-18T17:04:41.621512Z","shell.execute_reply":"2025-11-18T17:04:41.711839Z"}},"outputs":[{"name":"stdout","text":"Device = cuda\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"# LOAD EMBEDDINGS FROM PART 1","metadata":{}},{"cell_type":"code","source":"emb = np.load(\"/kaggle/input/custom-word2vec-output/embeddings.npy\")  \nwith open(\"/kaggle/input/custom-word2vec-output/word_to_idx.pkl\", \"rb\") as f:\n    word_to_idx = pickle.load(f)\n\norig_vocab_size, EMBED_DIM = emb.shape\nprint(\"Original vocab:\", orig_vocab_size, \"Embedding dim:\", EMBED_DIM)\n\n\nPAD_ID = 0\nUNK_ID = 1\n\nnew_emb = np.zeros((orig_vocab_size + 2, EMBED_DIM), dtype=np.float32)\nnew_emb[2:] = emb                              \nnew_emb[UNK_ID] = np.random.uniform(-0.01,0.01,EMBED_DIM)  \n\n# shift word_to_idx by +2\nnew_w2i = {\"<PAD>\":0, \"<UNK>\":1}\nfor w, i in word_to_idx.items():\n    new_w2i[w] = i + 2\n\nword_to_idx = new_w2i\nembedding_matrix = torch.tensor(new_emb, dtype=torch.float32)\nvocab_size = embedding_matrix.shape[0]\n\nprint(\"Final vocab with PAD/UNK:\", vocab_size)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-18T17:04:41.714331Z","iopub.execute_input":"2025-11-18T17:04:41.714658Z","iopub.status.idle":"2025-11-18T17:04:41.961409Z","shell.execute_reply.started":"2025-11-18T17:04:41.714632Z","shell.execute_reply":"2025-11-18T17:04:41.960596Z"}},"outputs":[{"name":"stdout","text":"Original vocab: 14068 Embedding dim: 100\nFinal vocab with PAD/UNK: 14070\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"# DATA PREPARATION","metadata":{}},{"cell_type":"code","source":"dataset = load_dataset(\"lhoestq/conll2003\")\n\ntrain_split = dataset[\"train\"]\nval_split = dataset[\"validation\"]\ntest_split = dataset[\"test\"]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-18T17:04:41.962449Z","iopub.execute_input":"2025-11-18T17:04:41.962926Z","iopub.status.idle":"2025-11-18T17:04:45.260885Z","shell.execute_reply.started":"2025-11-18T17:04:41.962901Z","shell.execute_reply":"2025-11-18T17:04:45.260174Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"dataset_infos.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0bf52fa55dea4297a441da4fbb7541c5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"data/train-00000-of-00001.parquet:   0%|          | 0.00/1.07M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2098321af47349f58fefd6a95717001c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"data/validation-00000-of-00001.parquet:   0%|          | 0.00/281k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f3ee91fe4a44446c8d8f53783cf37598"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"data/test-00000-of-00001.parquet:   0%|          | 0.00/259k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cd7fd60e5639442882c32558b1188265"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/14041 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d415434a7ce9418f95c53aa4b57a3f44"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/3250 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"79dc865f3b9445c496532b1a91556f9e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/3453 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f602e93e3c3046dca81b866c64fb1550"}},"metadata":{}}],"execution_count":3},{"cell_type":"markdown","source":"# WINDOW DATASET","metadata":{}},{"cell_type":"code","source":"class WindowNERDataset(Dataset):\n    def __init__(self, split, word_to_idx, window_size):\n        self.window_size = window_size\n        self.pad = word_to_idx[\"<PAD>\"]\n        self.unk = word_to_idx[\"<UNK>\"]\n        self.samples = []\n\n        for entry in split:\n            tokens = entry[\"tokens\"]\n            labels = entry[\"ner_tags\"]\n\n            idxs = [word_to_idx.get(t.lower(), self.unk) for t in tokens]\n\n            padded = [self.pad]*window_size + idxs + [self.pad]*window_size\n\n            for i in range(len(tokens)):\n                window = padded[i:i + 2*window_size + 1]\n                self.samples.append((window, labels[i]))\n\n    def __len__(self):\n        return len(self.samples)\n\n    def __getitem__(self, idx):\n        w, y = self.samples[idx]\n        return torch.tensor(w, dtype=torch.long), torch.tensor(y, dtype=torch.long)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-18T17:04:45.263062Z","iopub.execute_input":"2025-11-18T17:04:45.263560Z","iopub.status.idle":"2025-11-18T17:04:45.272656Z","shell.execute_reply.started":"2025-11-18T17:04:45.263533Z","shell.execute_reply":"2025-11-18T17:04:45.271734Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"# FEED FORWARD TAGGER","metadata":{}},{"cell_type":"code","source":"class FFNTagger(nn.Module):\n    def __init__(self, embedding_matrix, window=2, hidden=256, hidden2=128, num_classes=9):\n        super().__init__()\n\n        self.embedding = nn.Embedding.from_pretrained(\n            embedding_matrix,\n            freeze=True      \n        )\n\n        input_dim = (2*window + 1) * embedding_matrix.size(1)\n\n        self.ffn = nn.Sequential(\n            nn.Linear(input_dim, hidden),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n\n            nn.Linear(hidden, hidden2),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n\n            nn.Linear(hidden2, num_classes)\n        )\n\n    def forward(self, x):\n        emb = self.embedding(x)                      # (B, window, D)\n        concat = emb.view(emb.size(0), -1)           # (B, window*D)\n        return self.ffn(concat)                      # (B, num_classes)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-18T17:04:45.273628Z","iopub.execute_input":"2025-11-18T17:04:45.273889Z","iopub.status.idle":"2025-11-18T17:04:45.319384Z","shell.execute_reply.started":"2025-11-18T17:04:45.273866Z","shell.execute_reply":"2025-11-18T17:04:45.318519Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"# BUILD DATASETS & LOADERS","metadata":{}},{"cell_type":"code","source":"WINDOW = 2\nHIDDEN_DIM = 256\nNUM_CLASSES = 9\n\nBATCH_SIZE = 128\nEPOCHS = 60        \nLR = 3e-3          \n\ntrain_ds = WindowNERDataset(train_split, word_to_idx, WINDOW)\nval_ds   = WindowNERDataset(val_split, word_to_idx, WINDOW)\ntest_ds  = WindowNERDataset(test_split,  word_to_idx, WINDOW)\n\ntrain_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\nval_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE)\ntest_loader  = DataLoader(test_ds,  batch_size=BATCH_SIZE)\n\nprint(\"Train samples =\", len(train_ds))\nprint(\"Val samples   =\", len(val_ds))\nprint(\"Test samples  =\", len(test_ds))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-18T17:04:45.320279Z","iopub.execute_input":"2025-11-18T17:04:45.320544Z","iopub.status.idle":"2025-11-18T17:04:48.082879Z","shell.execute_reply.started":"2025-11-18T17:04:45.320500Z","shell.execute_reply":"2025-11-18T17:04:48.082033Z"}},"outputs":[{"name":"stdout","text":"Train samples = 203621\nVal samples   = 51362\nTest samples  = 46435\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"# TRAINING & EVALUATION","metadata":{}},{"cell_type":"code","source":"def evaluate(model, loader, loss_fn):\n    model.eval()\n    total_loss = 0\n    y_true, y_pred = [], []\n\n    with torch.no_grad():\n        for x, y in loader:\n            x, y = x.to(DEVICE), y.to(DEVICE)\n            logits = model(x)\n            loss = loss_fn(logits, y)\n            total_loss += loss.item()\n\n            preds = logits.argmax(1)\n            y_true += y.cpu().tolist()\n            y_pred += preds.cpu().tolist()\n\n    avg_loss = total_loss / len(loader)\n    report = classification_report(y_true, y_pred, digits=4)\n    return avg_loss, report\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-18T17:04:48.083720Z","iopub.execute_input":"2025-11-18T17:04:48.084064Z","iopub.status.idle":"2025-11-18T17:04:48.089326Z","shell.execute_reply.started":"2025-11-18T17:04:48.084036Z","shell.execute_reply":"2025-11-18T17:04:48.088571Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"# TRAINING LOOP","metadata":{}},{"cell_type":"code","source":"model = FFNTagger(embedding_matrix=embedding_matrix).to(DEVICE)\n\n\noptimizer = optim.Adam(model.parameters(), lr=LR)\nloss_fn = nn.CrossEntropyLoss()\n\n\nscheduler = optim.lr_scheduler.ReduceLROnPlateau(\n    optimizer,\n    mode='min',\n    factor=0.5,       \n    patience=3,       \n    verbose=True\n)\n\nbest_val_loss = float(\"inf\")\n\n\npatience_limit = 6\npatience_counter = 0\n\nfor epoch in range(1, EPOCHS+1):\n\n    model.train()\n    total_train = 0\n\n    for x, y in train_loader:\n        x, y = x.to(DEVICE), y.to(DEVICE)\n\n        optimizer.zero_grad()\n        logits = model(x)\n        loss = loss_fn(logits, y)\n        loss.backward()\n        optimizer.step()\n\n        total_train += loss.item()\n\n    avg_train_loss = total_train / len(train_loader)\n    avg_val_loss, val_report = evaluate(model, val_loader, loss_fn)\n\n    \n    scheduler.step(avg_val_loss)\n\n    print(f\"\\nEpoch {epoch}/{EPOCHS}\")\n    print(f\"Train Loss = {avg_train_loss:.4f}\")\n    print(f\"Val Loss   = {avg_val_loss:.4f}\")\n    print(\"Validation Report:\")\n    print(val_report)\n\n    if avg_val_loss < best_val_loss:\n        best_val_loss = avg_val_loss\n        patience_counter = 0                     \n        torch.save(model.state_dict(), \"best_ffn_model.pt\")\n        print(\" Saved best model!\")\n    else:\n        patience_counter += 1\n        print(f\"No improvement ({patience_counter}/{patience_limit})\")\n\n    # --- Early stopping break ---\n    if patience_counter >= patience_limit:\n        print(\"Early stopping triggered\")\n        break\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-18T17:04:48.090207Z","iopub.execute_input":"2025-11-18T17:04:48.090458Z","iopub.status.idle":"2025-11-18T17:07:37.796194Z","shell.execute_reply.started":"2025-11-18T17:04:48.090432Z","shell.execute_reply":"2025-11-18T17:07:37.795341Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 1/60\nTrain Loss = 0.3463\nVal Loss   = 0.2733\nValidation Report:\n              precision    recall  f1-score   support\n\n           0     0.9485    0.9789    0.9635     42759\n           1     0.7627    0.8377    0.7984      1842\n           2     0.8440    0.7950    0.8188      1307\n           3     0.8036    0.4303    0.5605      1341\n           4     0.7429    0.2770    0.4035       751\n           5     0.7236    0.8095    0.7641      1837\n           6     0.6126    0.5292    0.5678       257\n           7     0.6076    0.3796    0.4673       922\n           8     0.6719    0.2486    0.3629       346\n\n    accuracy                         0.9206     51362\n   macro avg     0.7464    0.5873    0.6341     51362\nweighted avg     0.9147    0.9206    0.9131     51362\n\n Saved best model!\n\nEpoch 2/60\nTrain Loss = 0.2818\nVal Loss   = 0.2610\nValidation Report:\n              precision    recall  f1-score   support\n\n           0     0.9331    0.9915    0.9614     42759\n           1     0.7899    0.8469    0.8174      1842\n           2     0.9187    0.7613    0.8326      1307\n           3     0.8324    0.4407    0.5763      1341\n           4     0.7424    0.2916    0.4187       751\n           5     0.8904    0.6456    0.7485      1837\n           6     0.8203    0.4086    0.5455       257\n           7     0.7622    0.2885    0.4186       922\n           8     0.8772    0.1445    0.2481       346\n\n    accuracy                         0.9222     51362\n   macro avg     0.8407    0.5355    0.6186     51362\nweighted avg     0.9167    0.9222    0.9107     51362\n\n Saved best model!\n\nEpoch 3/60\nTrain Loss = 0.2660\nVal Loss   = 0.2513\nValidation Report:\n              precision    recall  f1-score   support\n\n           0     0.9380    0.9935    0.9649     42759\n           1     0.8233    0.8274    0.8253      1842\n           2     0.9101    0.7819    0.8412      1307\n           3     0.7982    0.4571    0.5813      1341\n           4     0.8252    0.3395    0.4811       751\n           5     0.8889    0.7142    0.7920      1837\n           6     0.7458    0.5136    0.6083       257\n           7     0.8158    0.2690    0.4046       922\n           8     0.9531    0.1763    0.2976       346\n\n    accuracy                         0.9277     51362\n   macro avg     0.8554    0.5636    0.6440     51362\nweighted avg     0.9230    0.9277    0.9172     51362\n\n Saved best model!\n\nEpoch 4/60\nTrain Loss = 0.2537\nVal Loss   = 0.2438\nValidation Report:\n              precision    recall  f1-score   support\n\n           0     0.9440    0.9909    0.9669     42759\n           1     0.8314    0.8301    0.8308      1842\n           2     0.9088    0.8080    0.8554      1307\n           3     0.8300    0.4623    0.5939      1341\n           4     0.8498    0.2863    0.4283       751\n           5     0.8541    0.7550    0.8015      1837\n           6     0.8015    0.4241    0.5547       257\n           7     0.6887    0.4295    0.5291       922\n           8     0.7778    0.3237    0.4571       346\n\n    accuracy                         0.9305     51362\n   macro avg     0.8318    0.5900    0.6686     51362\nweighted avg     0.9251    0.9305    0.9223     51362\n\n Saved best model!\n\nEpoch 5/60\nTrain Loss = 0.2473\nVal Loss   = 0.2352\nValidation Report:\n              precision    recall  f1-score   support\n\n           0     0.9438    0.9920    0.9673     42759\n           1     0.8437    0.8089    0.8259      1842\n           2     0.8881    0.8256    0.8557      1307\n           3     0.8203    0.4631    0.5920      1341\n           4     0.8486    0.3582    0.5037       751\n           5     0.8648    0.7452    0.8006      1837\n           6     0.7435    0.5525    0.6339       257\n           7     0.8167    0.3720    0.5112       922\n           8     0.7283    0.3642    0.4855       346\n\n    accuracy                         0.9317     51362\n   macro avg     0.8331    0.6091    0.6862     51362\nweighted avg     0.9266    0.9317    0.9238     51362\n\n Saved best model!\n\nEpoch 6/60\nTrain Loss = 0.2402\nVal Loss   = 0.2333\nValidation Report:\n              precision    recall  f1-score   support\n\n           0     0.9446    0.9917    0.9675     42759\n           1     0.8682    0.8116    0.8389      1842\n           2     0.9155    0.8125    0.8610      1307\n           3     0.8147    0.4952    0.6160      1341\n           4     0.8118    0.4021    0.5378       751\n           5     0.8771    0.7501    0.8087      1837\n           6     0.7542    0.5253    0.6193       257\n           7     0.7534    0.4241    0.5427       922\n           8     0.8346    0.3208    0.4635       346\n\n    accuracy                         0.9334     51362\n   macro avg     0.8416    0.6148    0.6950     51362\nweighted avg     0.9282    0.9334    0.9263     51362\n\n Saved best model!\n\nEpoch 7/60\nTrain Loss = 0.2326\nVal Loss   = 0.2309\nValidation Report:\n              precision    recall  f1-score   support\n\n           0     0.9405    0.9937    0.9664     42759\n           1     0.8742    0.7959    0.8332      1842\n           2     0.8935    0.8409    0.8664      1307\n           3     0.8493    0.4832    0.6160      1341\n           4     0.8305    0.3262    0.4685       751\n           5     0.9085    0.7295    0.8092      1837\n           6     0.7697    0.5331    0.6299       257\n           7     0.8271    0.3579    0.4996       922\n           8     0.7588    0.3728    0.5000       346\n\n    accuracy                         0.9322     51362\n   macro avg     0.8502    0.6037    0.6877     51362\nweighted avg     0.9277    0.9322    0.9238     51362\n\n Saved best model!\n\nEpoch 8/60\nTrain Loss = 0.2297\nVal Loss   = 0.2282\nValidation Report:\n              precision    recall  f1-score   support\n\n           0     0.9452    0.9924    0.9683     42759\n           1     0.8543    0.8149    0.8341      1842\n           2     0.8586    0.8500    0.8543      1307\n           3     0.8554    0.4631    0.6009      1341\n           4     0.7808    0.3795    0.5108       751\n           5     0.8674    0.7512    0.8051      1837\n           6     0.7459    0.5253    0.6164       257\n           7     0.8628    0.3753    0.5231       922\n           8     0.8212    0.3584    0.4990       346\n\n    accuracy                         0.9334     51362\n   macro avg     0.8435    0.6122    0.6902     51362\nweighted avg     0.9289    0.9334    0.9255     51362\n\n Saved best model!\n\nEpoch 9/60\nTrain Loss = 0.2257\nVal Loss   = 0.2295\nValidation Report:\n              precision    recall  f1-score   support\n\n           0     0.9440    0.9927    0.9677     42759\n           1     0.8676    0.8073    0.8363      1842\n           2     0.9156    0.7965    0.8519      1307\n           3     0.9019    0.4661    0.6146      1341\n           4     0.8338    0.4008    0.5414       751\n           5     0.8709    0.7676    0.8160      1837\n           6     0.7829    0.5331    0.6343       257\n           7     0.8375    0.3970    0.5386       922\n           8     0.5802    0.4393    0.5000       346\n\n    accuracy                         0.9338     51362\n   macro avg     0.8371    0.6222    0.7001     51362\nweighted avg     0.9300    0.9338    0.9266     51362\n\nNo improvement (1/6)\n\nEpoch 10/60\nTrain Loss = 0.2238\nVal Loss   = 0.2278\nValidation Report:\n              precision    recall  f1-score   support\n\n           0     0.9462    0.9931    0.9691     42759\n           1     0.8559    0.8415    0.8486      1842\n           2     0.8818    0.8615    0.8715      1307\n           3     0.8782    0.5056    0.6417      1341\n           4     0.8168    0.4154    0.5508       751\n           5     0.8997    0.7621    0.8252      1837\n           6     0.8232    0.5253    0.6413       257\n           7     0.8305    0.3666    0.5087       922\n           8     0.9381    0.3064    0.4619       346\n\n    accuracy                         0.9367     51362\n   macro avg     0.8745    0.6197    0.7021     51362\nweighted avg     0.9333    0.9367    0.9292     51362\n\n Saved best model!\n\nEpoch 11/60\nTrain Loss = 0.2205\nVal Loss   = 0.2215\nValidation Report:\n              precision    recall  f1-score   support\n\n           0     0.9485    0.9924    0.9699     42759\n           1     0.8658    0.8474    0.8565      1842\n           2     0.8963    0.8401    0.8673      1307\n           3     0.8455    0.5183    0.6426      1341\n           4     0.8576    0.3848    0.5312       751\n           5     0.8764    0.7719    0.8208      1837\n           6     0.8246    0.5486    0.6589       257\n           7     0.8596    0.4317    0.5747       922\n           8     0.7717    0.4104    0.5358       346\n\n    accuracy                         0.9380     51362\n   macro avg     0.8607    0.6384    0.7175     51362\nweighted avg     0.9342    0.9380    0.9314     51362\n\n Saved best model!\n\nEpoch 12/60\nTrain Loss = 0.2172\nVal Loss   = 0.2230\nValidation Report:\n              precision    recall  f1-score   support\n\n           0     0.9463    0.9934    0.9693     42759\n           1     0.8721    0.8143    0.8422      1842\n           2     0.8760    0.8699    0.8729      1307\n           3     0.8669    0.5101    0.6423      1341\n           4     0.8551    0.3928    0.5383       751\n           5     0.8577    0.7910    0.8230      1837\n           6     0.9137    0.4942    0.6414       257\n           7     0.8753    0.3655    0.5157       922\n           8     0.8762    0.2659    0.4080       346\n\n    accuracy                         0.9365     51362\n   macro avg     0.8821    0.6108    0.6948     51362\nweighted avg     0.9334    0.9365    0.9287     51362\n\nNo improvement (1/6)\n\nEpoch 13/60\nTrain Loss = 0.2138\nVal Loss   = 0.2173\nValidation Report:\n              precision    recall  f1-score   support\n\n           0     0.9528    0.9902    0.9711     42759\n           1     0.8396    0.8751    0.8570      1842\n           2     0.8877    0.8646    0.8760      1307\n           3     0.7478    0.5682    0.6458      1341\n           4     0.8383    0.4141    0.5544       751\n           5     0.8917    0.7665    0.8244      1837\n           6     0.8395    0.5292    0.6492       257\n           7     0.8383    0.4273    0.5661       922\n           8     0.9453    0.3497    0.5105       346\n\n    accuracy                         0.9387     51362\n   macro avg     0.8645    0.6428    0.7172     51362\nweighted avg     0.9352    0.9387    0.9328     51362\n\n Saved best model!\n\nEpoch 14/60\nTrain Loss = 0.2123\nVal Loss   = 0.2195\nValidation Report:\n              precision    recall  f1-score   support\n\n           0     0.9487    0.9920    0.9699     42759\n           1     0.8851    0.8322    0.8579      1842\n           2     0.8978    0.8401    0.8680      1307\n           3     0.8084    0.5570    0.6596      1341\n           4     0.8151    0.4754    0.6005       751\n           5     0.8768    0.7752    0.8229      1837\n           6     0.7360    0.5642    0.6388       257\n           7     0.9106    0.3644    0.5205       922\n           8     0.8562    0.3613    0.5081       346\n\n    accuracy                         0.9381     51362\n   macro avg     0.8594    0.6402    0.7162     51362\nweighted avg     0.9346    0.9381    0.9317     51362\n\nNo improvement (1/6)\n\nEpoch 15/60\nTrain Loss = 0.2106\nVal Loss   = 0.2195\nValidation Report:\n              precision    recall  f1-score   support\n\n           0     0.9528    0.9899    0.9710     42759\n           1     0.8694    0.8567    0.8630      1842\n           2     0.8658    0.8837    0.8747      1307\n           3     0.7502    0.5712    0.6486      1341\n           4     0.8094    0.4581    0.5850       751\n           5     0.9035    0.7441    0.8161      1837\n           6     0.7637    0.5409    0.6333       257\n           7     0.8871    0.4089    0.5598       922\n           8     0.7014    0.4480    0.5467       346\n\n    accuracy                         0.9386     51362\n   macro avg     0.8337    0.6557    0.7220     51362\nweighted avg     0.9346    0.9386    0.9332     51362\n\nNo improvement (2/6)\n\nEpoch 16/60\nTrain Loss = 0.2072\nVal Loss   = 0.2231\nValidation Report:\n              precision    recall  f1-score   support\n\n           0     0.9502    0.9920    0.9707     42759\n           1     0.8646    0.8529    0.8587      1842\n           2     0.9169    0.8439    0.8789      1307\n           3     0.8578    0.5265    0.6525      1341\n           4     0.7618    0.5153    0.6148       751\n           5     0.8779    0.7556    0.8122      1837\n           6     0.8160    0.5175    0.6333       257\n           7     0.8427    0.4534    0.5896       922\n           8     0.8939    0.3410    0.4937       346\n\n    accuracy                         0.9393     51362\n   macro avg     0.8647    0.6442    0.7227     51362\nweighted avg     0.9356    0.9393    0.9334     51362\n\nNo improvement (3/6)\n\nEpoch 17/60\nTrain Loss = 0.2058\nVal Loss   = 0.2201\nValidation Report:\n              precision    recall  f1-score   support\n\n           0     0.9504    0.9923    0.9709     42759\n           1     0.8839    0.8350    0.8587      1842\n           2     0.8931    0.8562    0.8742      1307\n           3     0.8252    0.5421    0.6544      1341\n           4     0.8523    0.3995    0.5440       751\n           5     0.8910    0.7784    0.8309      1837\n           6     0.7291    0.5759    0.6435       257\n           7     0.8125    0.4512    0.5802       922\n           8     0.8343    0.4220    0.5605       346\n\n    accuracy                         0.9394     51362\n   macro avg     0.8524    0.6503    0.7241     51362\nweighted avg     0.9354    0.9394    0.9335     51362\n\nNo improvement (4/6)\n\nEpoch 18/60\nTrain Loss = 0.1858\nVal Loss   = 0.2100\nValidation Report:\n              precision    recall  f1-score   support\n\n           0     0.9533    0.9928    0.9726     42759\n           1     0.8819    0.8512    0.8663      1842\n           2     0.9072    0.8531    0.8793      1307\n           3     0.8594    0.5652    0.6820      1341\n           4     0.8481    0.4461    0.5846       751\n           5     0.8803    0.8084    0.8428      1837\n           6     0.7877    0.5486    0.6468       257\n           7     0.8630    0.4783    0.6155       922\n           8     0.8547    0.4249    0.5676       346\n\n    accuracy                         0.9431     51362\n   macro avg     0.8706    0.6632    0.7397     51362\nweighted avg     0.9398    0.9431    0.9378     51362\n\n Saved best model!\n\nEpoch 19/60\nTrain Loss = 0.1764\nVal Loss   = 0.2125\nValidation Report:\n              precision    recall  f1-score   support\n\n           0     0.9550    0.9918    0.9731     42759\n           1     0.8822    0.8496    0.8656      1842\n           2     0.9129    0.8500    0.8803      1307\n           3     0.7470    0.6100    0.6716      1341\n           4     0.8533    0.4647    0.6017       751\n           5     0.8803    0.7844    0.8296      1837\n           6     0.8654    0.5253    0.6538       257\n           7     0.8453    0.4740    0.6074       922\n           8     0.9000    0.3902    0.5444       346\n\n    accuracy                         0.9424     51362\n   macro avg     0.8713    0.6600    0.7364     51362\nweighted avg     0.9390    0.9424    0.9374     51362\n\nNo improvement (1/6)\n\nEpoch 20/60\nTrain Loss = 0.1740\nVal Loss   = 0.2112\nValidation Report:\n              precision    recall  f1-score   support\n\n           0     0.9535    0.9932    0.9730     42759\n           1     0.8743    0.8719    0.8731      1842\n           2     0.9286    0.8462    0.8855      1307\n           3     0.8521    0.5630    0.6780      1341\n           4     0.8450    0.4647    0.5997       751\n           5     0.8843    0.8073    0.8441      1837\n           6     0.7857    0.5564    0.6515       257\n           7     0.8901    0.4479    0.5960       922\n           8     0.8276    0.4162    0.5538       346\n\n    accuracy                         0.9436     51362\n   macro avg     0.8713    0.6630    0.7394     51362\nweighted avg     0.9405    0.9436    0.9382     51362\n\nNo improvement (2/6)\n\nEpoch 21/60\nTrain Loss = 0.1713\nVal Loss   = 0.2113\nValidation Report:\n              precision    recall  f1-score   support\n\n           0     0.9542    0.9923    0.9729     42759\n           1     0.8861    0.8572    0.8714      1842\n           2     0.9097    0.8554    0.8817      1307\n           3     0.8433    0.5697    0.6800      1341\n           4     0.8251    0.4900    0.6149       751\n           5     0.8769    0.8182    0.8465      1837\n           6     0.8797    0.5409    0.6699       257\n           7     0.8840    0.4544    0.6003       922\n           8     0.8216    0.4393    0.5725       346\n\n    accuracy                         0.9438     51362\n   macro avg     0.8756    0.6686    0.7456     51362\nweighted avg     0.9405    0.9438    0.9386     51362\n\nNo improvement (3/6)\n\nEpoch 22/60\nTrain Loss = 0.1701\nVal Loss   = 0.2118\nValidation Report:\n              precision    recall  f1-score   support\n\n           0     0.9533    0.9931    0.9728     42759\n           1     0.9035    0.8339    0.8673      1842\n           2     0.8776    0.8891    0.8833      1307\n           3     0.8355    0.5757    0.6817      1341\n           4     0.8927    0.4208    0.5719       751\n           5     0.8868    0.8057    0.8443      1837\n           6     0.7935    0.5681    0.6621       257\n           7     0.8642    0.4761    0.6140       922\n           8     0.8718    0.3931    0.5418       346\n\n    accuracy                         0.9433     51362\n   macro avg     0.8754    0.6617    0.7377     51362\nweighted avg     0.9403    0.9433    0.9378     51362\n\nNo improvement (4/6)\n\nEpoch 23/60\nTrain Loss = 0.1595\nVal Loss   = 0.2079\nValidation Report:\n              precision    recall  f1-score   support\n\n           0     0.9573    0.9919    0.9743     42759\n           1     0.8741    0.8779    0.8759      1842\n           2     0.9163    0.8630    0.8889      1307\n           3     0.8340    0.6070    0.7026      1341\n           4     0.8453    0.5020    0.6299       751\n           5     0.8894    0.8100    0.8479      1837\n           6     0.8295    0.5681    0.6744       257\n           7     0.8772    0.4805    0.6209       922\n           8     0.8020    0.4682    0.5912       346\n\n    accuracy                         0.9460     51362\n   macro avg     0.8695    0.6854    0.7562     51362\nweighted avg     0.9429    0.9460    0.9415     51362\n\n Saved best model!\n\nEpoch 24/60\nTrain Loss = 0.1547\nVal Loss   = 0.2094\nValidation Report:\n              precision    recall  f1-score   support\n\n           0     0.9576    0.9916    0.9743     42759\n           1     0.8779    0.8708    0.8744      1842\n           2     0.9134    0.8477    0.8794      1307\n           3     0.8303    0.6092    0.7028      1341\n           4     0.8854    0.4834    0.6253       751\n           5     0.8840    0.8176    0.8495      1837\n           6     0.8125    0.5564    0.6605       257\n           7     0.8399    0.5065    0.6319       922\n           8     0.7500    0.4769    0.5830       346\n\n    accuracy                         0.9456     51362\n   macro avg     0.8612    0.6845    0.7535     51362\nweighted avg     0.9424    0.9456    0.9413     51362\n\nNo improvement (1/6)\n\nEpoch 25/60\nTrain Loss = 0.1525\nVal Loss   = 0.2084\nValidation Report:\n              precision    recall  f1-score   support\n\n           0     0.9560    0.9925    0.9739     42759\n           1     0.8967    0.8621    0.8790      1842\n           2     0.9040    0.8722    0.8879      1307\n           3     0.8272    0.6174    0.7071      1341\n           4     0.8435    0.4953    0.6242       751\n           5     0.8975    0.8051    0.8488      1837\n           6     0.7956    0.5603    0.6575       257\n           7     0.8770    0.4794    0.6199       922\n           8     0.8773    0.4133    0.5619       346\n\n    accuracy                         0.9457     51362\n   macro avg     0.8750    0.6775    0.7511     51362\nweighted avg     0.9427    0.9457    0.9411     51362\n\nNo improvement (2/6)\n\nEpoch 26/60\nTrain Loss = 0.1510\nVal Loss   = 0.2095\nValidation Report:\n              precision    recall  f1-score   support\n\n           0     0.9567    0.9923    0.9742     42759\n           1     0.8811    0.8648    0.8729      1842\n           2     0.9231    0.8638    0.8925      1307\n           3     0.8469    0.6107    0.7097      1341\n           4     0.7931    0.5206    0.6286       751\n           5     0.8819    0.8133    0.8462      1837\n           6     0.8136    0.5603    0.6636       257\n           7     0.9011    0.4544    0.6042       922\n           8     0.8216    0.4393    0.5725       346\n\n    accuracy                         0.9457     51362\n   macro avg     0.8688    0.6800    0.7516     51362\nweighted avg     0.9426    0.9457    0.9410     51362\n\nNo improvement (3/6)\n\nEpoch 27/60\nTrain Loss = 0.1492\nVal Loss   = 0.2094\nValidation Report:\n              precision    recall  f1-score   support\n\n           0     0.9591    0.9909    0.9747     42759\n           1     0.8831    0.8692    0.8761      1842\n           2     0.9043    0.8745    0.8891      1307\n           3     0.8180    0.6234    0.7076      1341\n           4     0.8118    0.4940    0.6142       751\n           5     0.8801    0.8231    0.8506      1837\n           6     0.8268    0.5759    0.6789       257\n           7     0.8545    0.5033    0.6334       922\n           8     0.8235    0.4451    0.5779       346\n\n    accuracy                         0.9462     51362\n   macro avg     0.8624    0.6888    0.7558     51362\nweighted avg     0.9428    0.9462    0.9420     51362\n\nNo improvement (4/6)\n\nEpoch 28/60\nTrain Loss = 0.1444\nVal Loss   = 0.2131\nValidation Report:\n              precision    recall  f1-score   support\n\n           0     0.9568    0.9928    0.9745     42759\n           1     0.8816    0.8648    0.8731      1842\n           2     0.9178    0.8630    0.8896      1307\n           3     0.8276    0.6085    0.7013      1341\n           4     0.8462    0.4980    0.6270       751\n           5     0.8943    0.8013    0.8452      1837\n           6     0.8033    0.5720    0.6682       257\n           7     0.8772    0.4805    0.6209       922\n           8     0.7950    0.4595    0.5824       346\n\n    accuracy                         0.9459     51362\n   macro avg     0.8666    0.6823    0.7536     51362\nweighted avg     0.9426    0.9459    0.9413     51362\n\nNo improvement (5/6)\n\nEpoch 29/60\nTrain Loss = 0.1430\nVal Loss   = 0.2118\nValidation Report:\n              precision    recall  f1-score   support\n\n           0     0.9591    0.9912    0.9749     42759\n           1     0.8825    0.8730    0.8777      1842\n           2     0.9073    0.8837    0.8953      1307\n           3     0.8218    0.6189    0.7061      1341\n           4     0.8224    0.5180    0.6356       751\n           5     0.8906    0.8198    0.8537      1837\n           6     0.8066    0.5681    0.6667       257\n           7     0.8582    0.4989    0.6310       922\n           8     0.8235    0.4451    0.5779       346\n\n    accuracy                         0.9468     51362\n   macro avg     0.8636    0.6907    0.7577     51362\nweighted avg     0.9435    0.9468    0.9427     51362\n\nNo improvement (6/6)\nEarly stopping triggered\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"# FINAL TEST EVALUATION","metadata":{}},{"cell_type":"code","source":"print(\"\\nLoading best model...\")\nmodel.load_state_dict(torch.load(\"best_ffn_model.pt\"))\nmodel.to(DEVICE)\n\ntest_loss, test_report = evaluate(model, test_loader, loss_fn)\n\nprint(\"\\n======== FINAL TEST RESULTS ========\")\nprint(\"Test Loss =\", test_loss)\nprint(test_report)\nprint(\"====================================\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-18T17:07:37.797161Z","iopub.execute_input":"2025-11-18T17:07:37.797650Z","iopub.status.idle":"2025-11-18T17:07:38.539070Z","shell.execute_reply.started":"2025-11-18T17:07:37.797630Z","shell.execute_reply":"2025-11-18T17:07:38.538059Z"}},"outputs":[{"name":"stdout","text":"\nLoading best model...\n\n======== FINAL TEST RESULTS ========\nTest Loss = 0.24498711664435643\n              precision    recall  f1-score   support\n\n           0     0.9507    0.9877    0.9688     38323\n           1     0.8474    0.8108    0.8287      1617\n           2     0.8926    0.8054    0.8467      1156\n           3     0.7866    0.5924    0.6758      1661\n           4     0.7992    0.5102    0.6228       835\n           5     0.8826    0.7572    0.8151      1668\n           6     0.7756    0.6187    0.6883       257\n           7     0.7091    0.4444    0.5464       702\n           8     0.6512    0.5185    0.5773       216\n\n    accuracy                         0.9335     46435\n   macro avg     0.8106    0.6717    0.7300     46435\nweighted avg     0.9286    0.9335    0.9289     46435\n\n====================================\n","output_type":"stream"}],"execution_count":9}]}